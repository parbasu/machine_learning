{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the unlabeled dataset represented in below figure: you can clearly see $5$ blobs of instances. The $K$-Means algorithm is a simple algorithm is a simple algorithm capable of clustering this kind of dataset very quickly and efficiently, often in just a few iterations.\n",
    "\n",
    "![](unlabeled_dataset_five_blobs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a K-means clusterer on this dataset. It will try to find each blob's center and assign each instance to the closest blob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you have specify the number of clusters $k$ that the algorithm must find. In this example, it is pretty obvious from looking at the data that $k$ should be set to $5$, but in general it is not that easy. We will discuss that shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance was assigned to one of the $5$ clusters. In the context of clustering, an instance's label is the index of the cluster that this instance gets assigned to by the algorithm: this is not to be confused with the class labels in classification (remember that clustering is an unsupervised learning task). The KMeans instance preserves a copy of the labels of the instances it was trained on, available via the labels_ instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 1, ..., 2, 1, 4], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred is kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the $5$ centroids the algorithm found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.8003719 ,  1.30022776],\n",
       "       [ 0.20876306,  2.25551336],\n",
       "       [-2.79290307,  2.79641063],\n",
       "       [-1.46679593,  2.28585348],\n",
       "       [-2.80389191,  1.80053039]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can easily assign new instances to the cluster whose centroid is closest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plot the cluster's decision boundaries, you get a Voronoi tesellation (see below figure, where each centroid is represented with an $X$).\n",
    "\n",
    "![](k_means_decision_boundaries_voronoi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of the instances were clearly assigned to the appropriate cluster, but a few instances were probably mislabeled (especially near the boundary between the\n",
    "top left cluster and the central cluster). Indeed, the K-Means algorithm does not behave very well when the blobs have very different diameters since all it cares about when assigning an instance to a cluster is the distance to the centroid.\n",
    "\n",
    "Instead of assigning each instance to a single cluster, which is called hard clustering, it can be useful to just give each instance a score per cluster: this is called soft clustering.\n",
    "For example, the score can be the distance between the instance and the centroid, or conversely it can be a similarity score (or affinity) such as the Gaussian Radial Basis\n",
    "Function\n",
    "\n",
    "In the KMeans class, the transform() method measures the distance from each instance to every centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.88647951, 0.32995317, 2.9042344 , 1.49439034, 2.81097811],\n",
       "       [5.8424306 , 2.80290755, 5.84739223, 4.4759332 , 5.80731861],\n",
       "       [1.71145466, 3.29399768, 0.29040966, 1.69136631, 1.2153953 ],\n",
       "       [1.21626675, 3.21806371, 0.36159148, 1.54808703, 0.72644072]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does the algorithm work? Well it is really quite simple. Suppose you were\n",
    "given the centroids: you could easily label all the instances in the dataset by assigning\n",
    "each of them to the cluster whose centroid is closest. Conversely, if you were given all\n",
    "the instance labels, you could easily locate all the centroids by computing the mean of\n",
    "the instances for each cluster. But you are given neither the labels nor the centroids,\n",
    "so how can you proceed? Well, just start by placing the centroids randomly (e.g., by\n",
    "picking $k$ instances at random and using their locations as centroids). Then label the\n",
    "instances, update the centroids, label the instances, update the centroids, and so on\n",
    "until the centroids stop moving. The algorithm is guaranteed to converge in a finite\n",
    "number of steps (usually quite small), it will not oscillate forever. You can see the\n",
    "algorithm in action in the below figure: the centroids  are initialized randomly (top left),\n",
    "then the instances are labeled (top right), then the centroids are updated (center left),\n",
    "the instances are relabeled (center right), and so on. As you can see, in just $3$ iterations\n",
    "the algorithm has reached a clustering that seems close to optimal.\n",
    "\n",
    "![](k_means_algorithm.png)\n",
    "\n",
    "The computational complexity of the algorithm is generally linear\n",
    "with regards to the number of instances $m$, the number of clusters\n",
    "$k$ and the number of dimensions $n$. However, this is only true when\n",
    "the data has a clustering structure. If it does not, then in the worst\n",
    "case scenario the complexity can increase exponentially with the\n",
    "number of instances. In practice, however, this rarely happens, and\n",
    "K-Means is generally one of the fastest clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, although the algorithm is guaranteed to converge, it may not converge to the right solution (i.e., it may converge to a local optimum): this depends on the\n",
    "centroid initialization. For example, the below figure shows two sub-optimal solutions that the algorithm can converge to if you are not lucky with the random intialization step:\n",
    "\n",
    "![](suboptimal_solutions_unlucky_centroid_initialization.png)\n",
    "\n",
    "Let’s look at a few ways you can mitigate this risk by improving the centroid initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid Intialization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you happen to know approximately where the centroids should be (e.g., if you ran\n",
    "another clustering algorithm earlier), then you can set the init hyperparameter to a\n",
    "NumPy array containing the list of centroids, and set n_init to $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution is to run the algorithm multiple times with different random initializations\n",
    "and keep the best solution. This is controlled by the n_init hyperparameter:\n",
    "by default, it is equal to 10, which means that the whole algorithm described earlier\n",
    "actually runs 10 times when you call fit(), and Scikit-Learn keeps the best solution.\n",
    "But how exactly does it know which solution is the best? Well of course it uses a performance\n",
    "metric! It is called the model’s inertia: this is the mean squared distance\n",
    "between each instance and its closest centroid. It is roughly equal to $223.3$ for the\n",
    "model on the left of the above figure, $211.6$ for the model on the right of the above figure. The KMeans class runs the algorithm n_init times and keeps the model with the lowest inertia.\n",
    "\n",
    "If you are curious, a model's inertia is accessible via the inertia_ instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score() method returns the negative inertia. Why negative? Well, it is because a predictor's score() method always respect the \"greater is better\" rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important improvement to the K-Means algorithm, called K-Means+\\+, was proposed in a 2006 paper by David Arthur and Sergei Vassilvitskii: they introduced a smarter initialization step that tends to select centroids that are distant from one\n",
    "another, and this makes the K-Means algorithm much less likely to converge to a suboptimal solution. They showed that the additional computation required for the smarter initialization step is well worth it since it makes it possible to drastically\n",
    "reduce the number of times the algorithm needs to be run to find the optimal solution.\n",
    "Here is the K-Means++ initialization algorithm: (omitted for now)\n",
    "\n",
    "The KMeans class actually uses this initialization method by default. If you want to\n",
    "force it to use the original method (i.e., picking $k$ instances randomly to define the\n",
    "initial centroids), then you can set the init hyperparameter to \"random\". You will\n",
    "rarely need to do this.\n",
    "\n",
    "\n",
    "**Accelerated K-Means**\n",
    "\n",
    "Another important improvement to the K-Means algorithm was proposed in a 2003 paper by Charles Elkan. It considerably accelerates the algorithm by avoiding many\n",
    "unnecessary distance calculations: this is achieved by exploiting the triangle inequality (i.e., the straight line is always the shortest) and by keeping track of lower and\n",
    "upper bounds for distances between instances and centroids. This is the algorithm\n",
    "used by default by the KMeans class (but you can force it to use the original algorithm\n",
    "by setting the algorithm hyperparameter to \"full\", although you probably will\n",
    "never need to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Finding the Optimal Number of Clusters\n",
    "\n",
    "So far, we have set the number of clusters k to 5 because it was obvious by looking at\n",
    "the data that this is the correct number of clusters. But in general, it will not be so\n",
    "easy to know how to set k, and the result might be quite bad if you set it to the wrong\n",
    "value. For example, as you can see in the below figure, setting k to 3 or 8 results in fairly\n",
    "bad models:\n",
    "\n",
    "![](bad_choice_number_clusters.png)\n",
    "\n",
    "You might be thinking that we could just pick the model with the lowest inertia,\n",
    "right? Unfortunately, it is not that simple. The inertia for k=3 is 653.2, which is much\n",
    "higher than for k=5 (which was 211.6), but with k=8, the inertia is just 119.1. The\n",
    "inertia is not a good performance metric when trying to choose k since it keeps getting\n",
    "lower as we increase k. Indeed, the more clusters there are, the closer each\n",
    "instance will be to its closest centroid, and therefore the lower the inertia will be. Let’s\n",
    "plot the inertia as a function of k (see below figure)\n",
    "\n",
    "![](selecting_clusters_elbow_rule.png)\n",
    "\n",
    "As you can see, the inertia drops very quickly as we increase k up to 4, but then it\n",
    "decreases much more slowly as we keep increasing k. This curve has roughly the\n",
    "shape of an arm, and there is an “elbow” at k=4 so if we did not know better, it would\n",
    "be a good choice: any lower value would be dramatic, while any higher value would\n",
    "not help much, and we might just be splitting perfectly good clusters in half for no\n",
    "good reason.\n",
    "\n",
    "This technique for choosing the best value for the number of clusters is rather coarse.\n",
    "A more precise approach (but also more computationally expensive) is to use the silhouette\n",
    "score, which is the mean silhouette coefficient over all the instances. An instance’s\n",
    "silhouette coefficient is equal to (b – a) / max(a, b) where a is the mean distance\n",
    "to the other instances in the same cluster (it is the mean intra-cluster distance), and b\n",
    "is the mean nearest-cluster distance, that is the mean distance to the instances of the\n",
    "next closest cluster (defined as the one that minimizes b, excluding the instance’s own\n",
    "cluster). The silhouette coefficient can vary between -1 and +1: a coefficient close to\n",
    "+1 means that the instance is well inside its own cluster and far from other clusters,\n",
    "while a coefficient close to 0 means that it is close to a cluster boundary, and finally a\n",
    "coefficient close to -1 means that the instance may have been assigned to the wrong\n",
    "cluster. To compute the silhouette score, you can use Scikit-Learn’s silhou\n",
    "ette_score() function, giving it all the instances in the dataset, and the labels they\n",
    "were assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import silhouette_score\n",
    "# silhouette_score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compare the silhouette scores for different numbers of clusters (see below figure)\n",
    "\n",
    "![](silhouette_score_clusters_k.png)\n",
    "\n",
    "As you can see, this visualization is much richer than the previous one: in particular,\n",
    "although it confirms that k=4 is a very good choice, it also underlines the fact that\n",
    "k=5 is quite good as well, and much better than k=6 or 7. This was not visible when\n",
    "comparing inertias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more informative visualization is obtained when you plot every instance's silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called the *silhouette diagram* (see below figure)\n",
    "\n",
    "![](silhouette_analysis.png)\n",
    "\n",
    "The vertical dashed lines represent the silhouette score for each number of clusters.\n",
    "When most of the instances in a cluster have a lower coefficient than this score (i.e., if\n",
    "many of the instances stop short of the dashed line, ending to the left of it), then the\n",
    "cluster is rather bad since this means its instances are much too close to other clusters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\n",
    "k=5, the clusters look pretty good – most instances extend beyond the dashed line, to\n",
    "the right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\n",
    "is rather big, while when k=5, all clusters have similar sizes, so even though the overall\n",
    "silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\n",
    "to use k=5 to get clusters of similar sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of K-Means\n",
    "\n",
    "Despite its many merits, most notably being fast and scalable, K_Means is not perfect. As we saw, it is necessary to run the algorithm several tijmes to avoid sub-optimal solutions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes. For example, the figure below, shows how K-Means clusters a dataset containing three ellipsoidal clusters of different dimensions, densities and orientations:\n",
    "\n",
    "![](k_means_fail_ellipsoidal_blobs.png)\n",
    "\n",
    "As you can see, neither of these solutions are any good. The solution on the left is better, but it still chops off 25% of the middle cluster and assigns it to the cluster on\n",
    "the right. The solution on the right is just terrible, even though its inertia is lower. So depending on the data, different clustering algorithms may perform better. For example, on these types of elliptical clusters, Gaussian mixture models work great.\n",
    "\n",
    "It is important to scale the input features before you run K-Means,\n",
    "or else the clusters may be very stretched, and K-Means will perform\n",
    "poorly. Scaling the features does not guarantee that all the\n",
    "clusters will be nice and spherical, but it generally improves things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "This algorithm defines clusters as continuous regions of high density. It is actually quite simple:\n",
    "\n",
    "- For each instance, the algorithm counts how many instances are located within a small distance $\\epsilon$ from it. This region is called the instance's $\\epsilon$-neighborhood.\n",
    "\n",
    "- If an instance has at least min_samples instances in its $\\epsilon$-neighborhood (including itself), then it is considered a *core instance*. In other words, core instances are those that are located in dense regions.\n",
    "\n",
    "- All instances in the neighborhood of a core instance belong to the same cluster. This may include other core instances, therefore a long sequence of neighboring core instances forms a single cluster.\n",
    "\n",
    "- Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly.\n",
    "\n",
    "This algorithm works well if all the clusters are dense enough, and they are well separated by low-density regions. The DBSCAN class in Scikit-Learn is simple to use. Let's test it on the moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(eps=0.05)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps = 0.05, min_samples = 5)\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe labels of all the instances are now available in the labels_ instance variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5,  2,  3,  0,  2,  2,  2,  4,\n",
       "        2,  2,  4,  0,  5,  1,  0,  5,  5,  3,  2,  2,  2,  4,  5,  0,  6,\n",
       "        1, -1,  0,  0,  2,  3, -1,  3,  4, -1,  4,  4,  2,  0,  6,  2,  0,\n",
       "        2,  4,  0,  3,  3,  0,  3,  5,  2, -1,  3,  0,  5,  0,  0,  0,  2,\n",
       "        3,  0,  3,  6,  0,  2,  4,  4,  0,  3,  0,  4,  0,  3, -1,  3,  4,\n",
       "        0, -1,  2,  2,  4,  1,  6,  0,  5,  2,  6,  2,  3,  3, -1,  2, -1,\n",
       "        2,  4,  0,  0,  0,  2,  5,  2,  0,  2, -1,  2,  3,  1,  5,  2,  0,\n",
       "        4,  2,  1,  2,  4,  4,  0,  0,  6,  2,  2,  0,  2, -1,  2,  0,  3,\n",
       "        3,  5, -1,  3,  3,  5,  2,  4,  2,  2,  2,  4,  2,  4,  0, -1,  2,\n",
       "        1,  2,  6,  4,  2,  3,  2, -1,  2,  0,  0,  2,  0,  4,  0,  4,  4,\n",
       "        1,  6,  3,  2,  6,  1,  4,  4,  2,  0,  6,  0,  1,  0,  2,  5,  1,\n",
       "        1,  2,  2,  2,  2,  0,  5,  5,  1,  4,  5,  0,  3,  3,  0,  0,  0,\n",
       "        2,  0,  5,  2, -1,  2,  3,  5,  2,  2,  0,  4,  0,  4,  2,  2,  4,\n",
       "        2,  2,  3,  2,  3,  0,  1,  2,  0,  4, -1,  6,  0,  3,  3,  5, -1,\n",
       "        5,  4,  3,  5,  0,  2, -1,  3,  6,  3,  3,  4,  3,  2,  2,  2,  1,\n",
       "        4,  0,  2,  2, -1,  2,  6,  3,  4,  4,  2,  2,  1,  0,  1,  6,  3,\n",
       "        2,  4,  0,  3,  5,  0,  3,  2,  0,  2,  5,  4,  5,  2, -1,  6,  5,\n",
       "        2,  2,  2,  0,  6,  1,  6,  2,  1,  3,  5,  0,  1, -1,  1,  2,  2,\n",
       "        4, -1,  2,  0,  0,  0,  0,  2,  6,  6, -1,  1,  2,  5,  3,  5,  4,\n",
       "        0,  2,  0, -1,  6,  2,  3,  2,  4,  2,  0, -1,  4, -1,  0, -1,  0,\n",
       "        6,  1,  5,  3,  1,  1,  0,  2,  3,  2, -1,  4,  2,  3,  4,  5,  4,\n",
       "       -1,  4, -1,  1,  0,  5,  6,  5,  3,  2,  5,  1, -1,  5,  2,  5,  5,\n",
       "        3,  2,  6,  3,  2,  2,  5,  5,  2,  5,  5,  2, -1,  3,  2,  2,  5,\n",
       "        3,  0,  3,  0,  3,  1,  6,  2,  2,  3,  6,  0,  0,  0,  0,  2, -1,\n",
       "        3,  0, -1,  2,  2,  4,  2,  2,  0,  2,  0,  6,  3,  2,  6,  0,  6,\n",
       "        1,  3,  3,  0,  4,  4,  1,  4,  6,  4,  3,  5,  1,  2, -1,  5,  0,\n",
       "        2,  0,  0,  0,  0,  5,  0,  0,  0,  2,  1,  5,  5,  0,  2,  0,  1,\n",
       "        1,  3,  3,  0,  2,  1,  3,  3,  1,  0,  6,  1,  3,  2,  3,  4,  2,\n",
       "        2,  2,  0,  0,  0,  3, -1,  1,  0,  3,  3,  4,  5,  6, -1,  0,  2,\n",
       "        3,  6,  5,  6,  1,  2,  2,  0,  5,  1,  2,  2,  2,  0,  1,  2,  2,\n",
       "        4,  0,  4,  3,  4,  2,  5, -1,  2,  3,  1,  0, -1,  4,  1,  0,  2,\n",
       "       -1,  0,  3,  0,  3,  0,  4,  2,  6,  5,  3,  2,  6,  5,  4,  5, -1,\n",
       "        3,  2,  0,  2,  0,  3,  3,  6,  0,  3,  2,  2,  1,  4,  3,  1,  5,\n",
       "       -1,  2,  2,  4,  6,  0,  4,  4,  2,  5,  3,  2,  3,  4,  2,  5,  6,\n",
       "        1,  0,  4,  3, -1,  2,  2,  1,  0, -1,  3,  5,  4,  2,  0,  2,  2,\n",
       "        5,  4,  0,  0,  2,  3,  3,  4,  4,  0,  2,  5,  3,  0, -1,  5,  4,\n",
       "       -1,  2,  2,  0,  2,  0, -1, -1,  2, -1, -1,  5,  3,  0,  3,  2,  2,\n",
       "        0,  2,  6,  4,  2,  2,  5,  3,  2,  6,  2,  1,  5,  3,  2,  1,  6,\n",
       "        2,  2,  2,  2,  5,  2,  2,  0, -1,  2,  0,  2,  5,  2, -1,  2,  2,\n",
       "        2,  3,  4,  0,  3,  1, -1,  2,  0,  2,  0, -1,  3,  5,  1,  2,  5,\n",
       "        0,  2,  2,  3,  2,  3,  6, -1,  4,  4,  2,  0,  0, -1,  2,  4,  0,\n",
       "        4,  1,  5,  2, -1, -1,  0,  2,  6,  5,  3,  2,  2,  2,  0,  1,  6,\n",
       "        4,  5,  3,  2,  0,  5, -1,  5,  0,  4,  4,  1,  3,  5,  5,  6,  4,\n",
       "        1,  2,  2,  2,  2,  2,  3,  0,  1,  2,  2,  2,  2,  5,  0,  2,  2,\n",
       "        2,  2,  0,  2,  2,  1,  2,  0,  4,  2,  6,  6,  3,  0,  4,  2, -1,\n",
       "        0,  1,  2,  2,  5,  2,  0,  4,  0,  1,  2,  0,  1, -1, -1,  0,  0,\n",
       "        4,  1,  1,  5,  2,  2,  6,  4, -1,  0,  2,  2,  2,  0,  2,  2,  0,\n",
       "        2,  0,  2,  0,  2,  3,  4,  0,  5,  0,  3,  0,  5,  0,  2,  2,  2,\n",
       "        2,  2,  3,  0,  4,  2,  0,  0,  6,  0,  2,  5,  1,  2,  1,  2,  2,\n",
       "        5, -1,  2,  0,  2,  0,  2,  0,  2,  0,  3, -1,  2,  0,  1,  0,  3,\n",
       "        3,  2,  2,  5,  5,  3,  2,  5,  6, -1,  1,  4,  1,  0,  1,  0,  6,\n",
       "        3,  3,  6,  0,  3,  2,  0,  2, -1,  3,  2,  3,  5,  3,  6,  1,  5,\n",
       "        2,  5,  2,  3,  2,  4,  2,  2,  2,  4,  0,  4,  2,  6,  6,  2, -1,\n",
       "        2,  4,  5,  5,  0,  2,  1,  0,  6,  2,  2,  3,  0,  1, -1,  2,  2,\n",
       "        0,  3,  5,  3,  2,  0,  1,  6,  1, -1,  4,  2,  0,  4,  2,  4,  3,\n",
       "        4, -1,  1,  4,  5,  2,  2,  0,  2,  6,  2,  0,  2,  4,  0,  2,  0,\n",
       "        0,  5,  2, -1,  6, -1,  0,  3,  6,  0,  2,  2,  4,  3,  4,  6,  5,\n",
       "        4,  2,  5,  1, -1,  1,  6,  4,  2,  4,  0,  5,  0,  2,  3,  1,  3,\n",
       "        6,  4,  5, -1,  3, -1,  3,  2,  3,  3,  4,  2,  6,  3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some instances have a cluster index equal to $-1$: this means that they are considered as anomalies by the algorithm. The indices of the core instances are available in the core_sample_indices_ instance variable, and the core instances themselves are available in the core_sample_indices_ instance variable, and the core instances themselves are available in the components_ instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dbscan.core_sample_indices_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   4,   5,   6,   7,   8,  10,  11,  12,  13,  14,  16,  17,\n",
       "        18,  19,  20,  21,  22,  23,  24,  25,  26,  28,  29,  30,  31,\n",
       "        32,  33,  34,  36,  38,  39,  41,  42,  44,  45,  47,  49,  50,\n",
       "        51,  52,  53,  54,  55,  56,  58,  59,  61,  63,  64,  65,  66,\n",
       "        67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
       "        80,  81,  83,  84,  85,  87,  88,  89,  90,  91,  93,  94,  96,\n",
       "        97,  98, 102, 103, 104, 105, 106, 107, 108, 109, 110, 113, 114,\n",
       "       115, 116, 117, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 135, 136, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149,\n",
       "       150, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164,\n",
       "       165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178,\n",
       "       179, 181, 182, 183, 185, 186, 187, 188, 189, 191, 193, 194, 195,\n",
       "       196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 209, 210,\n",
       "       211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224,\n",
       "       226, 228, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241,\n",
       "       242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256,\n",
       "       257, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271,\n",
       "       272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 300, 301, 303,\n",
       "       304, 305, 308, 309, 310, 311, 313, 315, 317, 318, 319, 320, 321,\n",
       "       322, 323, 324, 327, 328, 329, 330, 332, 333, 335, 339, 340, 341,\n",
       "       342, 343, 346, 347, 348, 349, 351, 352, 353, 354, 355, 356, 358,\n",
       "       360, 361, 362, 363, 364, 365, 366, 367, 368, 370, 371, 372, 373,\n",
       "       374, 375, 377, 378, 379, 380, 381, 382, 384, 385, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 397, 398, 399, 400, 401, 402, 403,\n",
       "       404, 405, 406, 409, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "       420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
       "       433, 435, 436, 437, 438, 440, 441, 442, 443, 444, 445, 446, 447,\n",
       "       448, 449, 450, 451, 452, 453, 454, 456, 457, 458, 459, 461, 462,\n",
       "       463, 464, 467, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "       479, 480, 483, 484, 485, 486, 487, 488, 489, 491, 492, 493, 495,\n",
       "       496, 497, 498, 499, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
       "       510, 511, 512, 513, 514, 515, 516, 518, 519, 520, 521, 523, 524,\n",
       "       525, 526, 528, 529, 530, 531, 532, 533, 535, 536, 537, 538, 539,\n",
       "       540, 541, 542, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553,\n",
       "       554, 555, 556, 557, 559, 560, 562, 563, 564, 565, 566, 568, 569,\n",
       "       570, 572, 573, 574, 575, 576, 578, 579, 580, 583, 584, 585, 586,\n",
       "       588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599, 600, 601,\n",
       "       602, 603, 604, 605, 607, 610, 611, 614, 615, 616, 617, 620, 624,\n",
       "       625, 627, 628, 629, 631, 633, 634, 635, 636, 637, 638, 639, 640,\n",
       "       641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 652, 655, 656,\n",
       "       657, 661, 662, 663, 664, 666, 667, 670, 671, 672, 673, 675, 676,\n",
       "       677, 678, 679, 680, 681, 682, 684, 685, 686, 688, 689, 690, 691,\n",
       "       692, 694, 695, 696, 697, 698, 703, 704, 705, 706, 708, 709, 710,\n",
       "       711, 712, 713, 714, 716, 717, 718, 719, 721, 722, 723, 724, 726,\n",
       "       729, 730, 731, 733, 735, 736, 737, 738, 739, 740, 741, 742, 743,\n",
       "       744, 745, 746, 748, 749, 750, 751, 752, 753, 754, 756, 757, 758,\n",
       "       759, 760, 761, 762, 763, 765, 766, 768, 770, 772, 773, 774, 775,\n",
       "       776, 777, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 791,\n",
       "       792, 793, 794, 795, 796, 797, 798, 799, 800, 802, 803, 804, 805,\n",
       "       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n",
       "       819, 820, 821, 822, 824, 825, 826, 827, 828, 829, 830, 831, 832,\n",
       "       835, 836, 837, 838, 839, 840, 841, 842, 843, 845, 846, 848, 849,\n",
       "       850, 851, 852, 853, 854, 855, 857, 858, 860, 861, 862, 863, 864,\n",
       "       865, 867, 868, 870, 871, 873, 877, 878, 879, 880, 882, 883, 884,\n",
       "       885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897,\n",
       "       898, 899, 902, 903, 904, 905, 906, 907, 908, 910, 912, 913, 916,\n",
       "       918, 919, 920, 921, 922, 923, 925, 926, 928, 929, 930, 931, 932,\n",
       "       933, 934, 935, 937, 938, 939, 940, 941, 942, 943, 944, 945, 947,\n",
       "       948, 949, 951, 952, 953, 954, 956, 958, 959, 960, 961, 962, 963,\n",
       "       964, 965, 966, 967, 968, 969, 970, 972, 974, 975, 976, 978, 979,\n",
       "       980, 982, 983, 984, 985, 986, 987, 988, 990, 992, 993, 995, 997,\n",
       "       998, 999])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.core_sample_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02137124,  0.40618608],\n",
       "       [-0.84192557,  0.53058695],\n",
       "       [ 0.58930337, -0.32137599],\n",
       "       ...,\n",
       "       [ 1.66258462, -0.3079193 ],\n",
       "       [-0.94355873,  0.3278936 ],\n",
       "       [ 0.79419406,  0.60777171]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering is represented in the left plot of the below figure. As you can see, it idenfied a lot anomalies, plus $7$ different clusters. How disappoingting! Fortunately, if we widen each instance's neighborhood by increasing eps to $0.2$, we get the clustering on the right, which looks perfect. Let's continue with this model.\n",
    "\n",
    "![](DBSCAN_different_radiuses.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Somewhat suprisingly, the DBSCAN class does not have a predict() method, although it has a fit_predict() method. In other words, it cannot predict which cluster a new instance belongs to. The rationale for this decision is that several classification algorithms could make sense here, and it is easy enough to train one, for example a KneighborsClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 50)\n",
    "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given a few new instances, we can predict in which cluster they most likely belong to, and even estimate a probability for each cluster. Note that we only trained them on the core instances, but we could also have chosen to train them on all the instances, or all but the anamalies: this choice depends on the final task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 3, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
    "knn.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24, 0.  , 0.  , 0.  , 0.  , 0.  , 0.76],\n",
       "       [1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.3 , 0.7 , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is represented on figure below (the crosses represent the $4$ instances in X_new). Notice that since there is no anomaly in the KNN’s training set, the classifier always chooses a cluster, even when that cluster is far away. However, it is fairly straightforward to introduce a maximum distance, in which case the two instances that are far away from both clusters are classified as anomalies. To do this, we can use the kneighbors() method of the KNeighborsClassifier: given a set of instances, it returns the distances and the indices of the $k$ nearest neighbors in the\n",
    "training set (two matrices, each with $k$ columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  3, -1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n",
    "y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n",
    "y_pred[y_dist > 0.2] = -1\n",
    "y_pred.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cluster_classification_diagram.png)\n",
    "\n",
    "In short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any number of clusters, of any shape, it is robust to outliers, and it has just two hyperparameters (eps and min_samples). However, if the densities varies significantly across the clusters, it can impossible for it capture all the clusters properly. Moreover, its computationally complexity is roughly $O(m \\log m)$, making it pretty close to linear with regards to the number of instances. However, Scikit-Learn's implementation can require up to $O(m^2)$ memory if eps is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *Gaussian mixture model* (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distributiion form a cluster that typically looks like an ellipsoid, Each cluster can have a different ellipsoidal shape, size, density and orientation, just like in the earlier figure. When you observe an instance, you know it was generated from one of the Gaussian distributions, but you are not told which one, and you do not know what the parameters of these distributions are.\n",
    "\n",
    "There are several GMM variants: in the simplest variant, implemented in the GaussianMixture class, you must know in advance the number $k$ of Gaussian distributions. The dataset $X$ is assumed to have been generated through the following probabilistic process:\n",
    "\n",
    "- For each instance, a cluster is picked randomly among $k$ clusters. The probability of choosing the $j$th cluster is deined by the cluster's weights $\\phi^{(j)}$. The index of the custer chosen for the $i$th instance is noted $z^{(i)}$.\n",
    "\n",
    "- If $z^{(i)} = j$, meaning the $i$th instance has been assigned to the $j$th cluster, the location $x^{(i)}$ of this instance is sampled randomly from the Gaussian distribution with mean $\\mu^{(j)}$ and covariance matrix $\\Sigma^{(j)}$. This is noted $x^{(i)} \\sim \\mathcal{N}(\\mu^{(i)}, \\Sigma^{(j)})$.\n",
    "\n",
    "So what can you do with such a model? Well, given the dataset $X$ you typically want to start by estimating the weights $\\phi$ and all the distribution parameters $\\mu^{(i)}$ to $\\mu^{(k)}$ and $\\Sigma^{(1)}$ to $\\Sigma^{(k)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(n_components=3, n_init=10, random_state=42)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm = GaussianMixture(n_components=3, n_init=10, random_state=42)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the parameters that the algorithm estimated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39054348, 0.2093669 , 0.40008962])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05224874,  0.07631976],\n",
       "       [ 3.40196611,  1.05838748],\n",
       "       [-1.40754214,  1.42716873]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.6890309 ,  0.79717058],\n",
       "        [ 0.79717058,  1.21367348]],\n",
       "\n",
       "       [[ 1.14296668, -0.03114176],\n",
       "        [-0.03114176,  0.9545003 ]],\n",
       "\n",
       "       [[ 0.63496849,  0.7298512 ],\n",
       "        [ 0.7298512 ,  1.16112807]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how? This class relies on the **Expectation-\n",
    "Maximization** (EM) algorithm (https://stephens999.github.io/fiveMinuteStats/intro_to_em.html), which has many similarities with the K-Means algorithm (details later!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check whether or not the algorithm converged and how many iterations it\n",
    "took:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that you have an estimate of the location, size, shape, orientation and relative\n",
    "weight of each cluster, the model can easily assign each instance to the most likely\n",
    "cluster (hard clustering) or estimate the probability that it belongs to a particular\n",
    "cluster (soft clustering). For this, just use the predict() method for hard clustering,\n",
    "or the predict_proba() method for soft clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.77227791e-01, 2.27715290e-02, 6.79898914e-07],\n",
       "       [9.83288385e-01, 1.60345103e-02, 6.77104389e-04],\n",
       "       [7.51824662e-05, 1.90251273e-06, 9.99922915e-01],\n",
       "       ...,\n",
       "       [4.35053542e-07, 9.99999565e-01, 2.17938894e-26],\n",
       "       [5.27837047e-16, 1.00000000e+00, 1.50679490e-41],\n",
       "       [2.32355608e-15, 1.00000000e+00, 8.21915701e-41]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a generative model, meaning you can actually sample new instances from it (note\n",
    "that they are ordered by cluster index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new, y_new = gm.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8690223 , -0.32680051],\n",
       "       [ 0.29945755,  0.2841852 ],\n",
       "       [ 1.85027284,  2.06556913],\n",
       "       [ 3.98260019,  1.50041446],\n",
       "       [ 3.82006355,  0.53143606],\n",
       "       [-1.04015332,  0.7864941 ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to estimate the density of the model at any given location. This is\n",
    "achieved using the score_samples() method: for each instance it is given, this\n",
    "method estimates the log of the probability density function (PDF) at that location.\n",
    "The greater the score, the higher the density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.60674489, -3.57074133, -3.33007348, ..., -3.51379355,\n",
       "       -4.39643283, -3.8055665 ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.score_samples(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the cluster means, the decision boundaries (dashed lines), and the density contours of this model:\n",
    "\n",
    "![](density_contours_gaussian_mixture.png)\n",
    "\n",
    "Nice! The algorithm clearly found an excellent solution. Of course, we made its task\n",
    "easy by actually generating the data using a set of 2D Gaussian distributions (unfortunately,\n",
    "real life data is not always so Gaussian and low-dimensional), and we also gave\n",
    "the algorithm the correct number of clusters. When there are many dimensions, or\n",
    "many clusters, or few instances, EM can struggle to converge to the optimal solution.\n",
    "You might need to reduce the difficulty of the task by limiting the number of parameters\n",
    "that the algorithm has to learn: one way to do this is to limit the range of shapes\n",
    "and orientations that the clusters can have. This can be achieved by imposing constraints\n",
    "on the covariance matrices. To do this, just set the covariance_type hyperparameter\n",
    "to one of the following values:\n",
    "\n",
    "\n",
    "- \"spherical\": all clusters must be spherical, but they can have different diameters\n",
    "(i.e., different variances).\n",
    "\n",
    "- \"diag\": clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s\n",
    "axes must be parallel to the coordinate axes (i.e., the covariance matrices must be\n",
    "diagonal).\n",
    "\n",
    "- \"tied\": all clusters must have the same ellipsoidal shape, size and orientation\n",
    "(i.e., all clusters share the same covariance matrix).\n",
    "\n",
    "By default, covariance_type is equal to \"full\", which means that each cluster can\n",
    "take on any shape, size and orientation (it has its own unconstrained covariance\n",
    "matrix). The below figure plots the solutions found by the EM algorithm when cova\n",
    "riance_type is set to \"tied\" or \"spherical“.\n",
    "\n",
    "![](covariance_type_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection using Gaussian Mixtures\n",
    "\n",
    "Anomaly detection (also called outlier detection) is the task of detecting instances that\n",
    "deviate strongly from the norm. These instances are of course called anomalies or\n",
    "outliers, while the normal instances are called inliers. Anomaly detection is very useful\n",
    "in a wide variety of applications, for example in fraud detection, or for detecting\n",
    "defective products in manufacturing, or to remove outliers from a dataset before\n",
    "training another model, which can significantly improve the performance of the\n",
    "resulting model.\n",
    "Using a Gaussian mixture model for anomaly detection is quite simple: any instance\n",
    "located in a low-density region can be considered an anomaly. You must define what\n",
    "density threshold you want to use. For example, in a manufacturing company that\n",
    "tries to detect defective products, the ratio of defective products is usually wellknown.\n",
    "Say it is equal to 4%, then you can set the density threshold to be the value\n",
    "that results in having 4% of the instances located in areas below that threshold density.\n",
    "If you notice that you get too many false positives (i.e., perfectly good products\n",
    "that are flagged as defective), you can lower the threshold. Conversely, if you have too\n",
    "many false negatives (i.e., defective products that the system does not flag as defective),\n",
    "you can increase the threshold. This is the usual precision/recall tradeoff. Here is how you would identify the outliers using the 4th percentile lowest density as the threshold (that is, approximately $4$% of the instances will be flagged as anomalies):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = gm.score_samples(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_threshold = np.percentile(densities,4)\n",
    "anomalies = X[densities < density_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These anamolies are represented as stars on the below figure.\n",
    "\n",
    "![](anomaly_detection_gaussian_mixture_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closely related task is novelty detection: it differs from anomaly detection in that the\n",
    "algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\n",
    "whereas anomaly detection does not make this assumption. Indeed, outlier detection\n",
    "is often precisely used to clean up a dataset.\n",
    "\n",
    "Gaussian mixture models try to fit all the data, including the outliers,\n",
    "so if you have too many of them, this will bias the model’s view\n",
    "of “normality”: some outliers may wrongly be considered as normal.\n",
    "If this happens, you can try to fit the model once, use it to\n",
    "detect and remove the most extreme outliers, then fit the model\n",
    "again on the cleaned up dataset. Another approach is to use robust\n",
    "covariance estimation methods (see the EllipticEnvelope class).\n",
    "\n",
    "Just like K-Means, the GaussianMixture algorithm requires you to specify the number\n",
    "of clusters. So how can you find it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Number of Clusters\n",
    "\n",
    "\n",
    "With K-Means, you could use the inertia or the silhouette score to select the appropriate\n",
    "number of clusters, but with Gaussian mixtures, it is not possible to use these\n",
    "metrics because they are not reliable when the clusters are not spherical or have different\n",
    "sizes. Instead, you can try to find the model that minimizes a theoretical information criterion such as the Bayesian information criterion (BIC) or the Akaike\n",
    "information criterion (AIC). (details omitted)\n",
    "\n",
    "Both the BIC and the AIC penalize models that have more parameters to learn (e.g.,\n",
    "more clusters), and reward models that fit the data well. They often end up selecting\n",
    "the same model, but when they differ, the model selected by the BIC tends to be simpler\n",
    "(fewer parameters) than the one selected by the AIC, but it does not fit the data\n",
    "quite as well (this is especially true for larger datasets).\n",
    "\n",
    "To compute the BIC and AIC, just call the bic() or aic() methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189.662685850679"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.bic(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8102.437405735641"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.aic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below figure shows the BIC for different numbers of clusters k. As you can see, both\n",
    "the BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\n",
    "that we could also search for the best value for the covariance_type hyperparameter.\n",
    "For example, if it is \"spherical\" rather than \"full\", then the model has much fewer\n",
    "parameters to learn, but it does not fit the data as well.\n",
    "\n",
    "![](aic_bic_different_numbers_clusters_k.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
