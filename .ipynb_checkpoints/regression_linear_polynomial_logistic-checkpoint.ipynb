{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant term called the *bias term* (also called the *intercept term*).\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n,  $$\n",
    "\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "\n",
    "- $n$ is the number of features.\n",
    "\n",
    "- $x_i$ is the the i-th feature value. \n",
    "\n",
    "- $\\theta_j$ is the j-th model parameter (including the bias term $\\theta_0$ and feature weights $\\theta_0, \\dots, \\theta_n$).\n",
    "\n",
    "We can write the above equation in vectorized form\n",
    "\n",
    "$$\\hat{y} = h_{\\theta}(x) = \\theta^T x,$$\n",
    "\n",
    "- $\\theta$ is the model's *parameter vector*, containing the bias term $\\theta_0$ and the feature weights $\\theta_1$ to $\\theta_n$.\n",
    "\n",
    "- $x$ is the instance's *feature vector*, containing $x_0$ to $x_n$, with $x_0$ always equal to $1$.\n",
    "\n",
    "- $\\theta$ and $x$ are column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is linear regression model. So how do we train it? Training a model means setting its paramters so that the model best fits the training set. For this purpose, we first need a measure of how well (or poorly) the model fits the training data. The most common performance measure of a regression model is the Root Mean Square Error (RMSE). Therefore, to train a Linear Regression mode, you need to find the value of $\\theta$ that minimize the RMSE. In practice is simpler to minimize the Mean Square Error (MSE) (= RMSE^2) than the RMSE, and it leads to the same result (because the value that minimizes a function also minimizes its square root)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE of Linear Regression hypothesis $h_\\theta$ on a training set $X$ is calculated using\n",
    "\n",
    "$$\\text{MSE}(X, h_\\theta) = \\frac{1}{m} \\sum_{i =1}^{m} (\\theta^T x^{(i)} - y^{(i)})^2.$$\n",
    "\n",
    "To simplify notations we will write just $\\text{MSE}(\\theta)$ instead of $\\text{MSE}(X, h_\\theta) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Normal Equation\n",
    "\n",
    "To find the value of $\\theta$ that minimizes the cost function, there is a *closed-form* solution. This is called the *Normal Equation*\n",
    "\n",
    "$$\\hat{\\theta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "- $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "\n",
    "- $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100,1) \n",
    "\n",
    "y = 4 + 3*X + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute $\\hat{\\theta}$ using the Normal Equation. We will use the inv() function from NumPy's Linear Algebra module (np.linalg) to compute the inverse of a matrix, and the dot() method for matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100,1)), X] #add x0 = 1 to each instance\n",
    "\n",
    "theta_best  = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function that we used to generate the data is $y = 4 +3x_1 +\\text{ Gaussian noise}$. Let's see what the equation found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.92675368],\n",
       "       [3.15085769]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have hoped for $\\theta_0 = 4$ and $\\theta_1 = 3$ instead of the above. Close enough, but the noise made it impossible to recover the exact parameters of the original function.\n",
    "\n",
    "Now you can make prediction using $\\hat{\\theta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.92675368],\n",
       "       [10.22846906]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0],[2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new] #add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perfom linear regression using Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.92675368]), array([[3.15085769]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X,y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for \"least squares\") which you could call directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.92675368],\n",
       "       [3.15085769]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond = 1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function computes $\\hat{\\theta} = X^{+}y$, where $X^{+}$ is the *pseudoinverse* of $X$ (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.92675368],\n",
       "       [3.15085769]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudoinverse itself is computed using a standard matrix factorization technique called *Singular Value Decompostion (SVD)* that can decompose the training  set matrix $X$ into the matrix multiplication of three matrices $U  \\ \\Sigma \\ V^T$ (see numpy.linalg.svd()). The pseudoinverse is computed as $X^{+} = V \\Sigma^{+} U^T$. To compute the matrix $\\Sigma^{+}$, the algorithm takes $\\Sigma$ and sets to zero all values smaller than a tiny threshold value, then it replaces all the non-zero values with their inverse, and finally it transposes the resulting matrix. This approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may not work if the matrix $X^T X$ is not invertible, such as if $m < n$ or if some features are redundant, but the pseudoinverse is always defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation complexity \n",
    "\n",
    "The Normal Equation computes the inverse of $X^T X$ which is an $(n+1) \\times (n+1)$ matrix (where $n$ is the number of features). The *computational complexity* of inverting such a matrix is typically about $O(n^{2.4})$ or $O(n^3)$ (depending on the implementation). The SVD approach used by SciKit-Learn's LinearRegression class is about $O(n^2)$. Also, once you have trained your Linear Regression model (using the Normal Equation or any other alogorithm), predictions are very fast: the computation complexity is linear with regards to both the number of instances you want to make predictions on and the number of features. In other words, making predictions on twice as many instances (or twice as many features) will just take roughly twice as much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look at very different ways to train a Linear Regression model, better\n",
    "suited for cases where there are a large number of features, or too many training\n",
    "instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general ideal of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. \n",
    "\n",
    "Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get the bottom of the valley quickly is to go downhill in the direction of the steepest slope. This is exactly what Gradient descent does: it measures the local gradient of the error function with regards to the parameter vector $\\theta$, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached the minimum!\n",
    "\n",
    "Concretely, you start by filling $\\theta$ with random values (this is *random initialization*), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., MSE), until the algorithm *converges* ot a minimum\n",
    "\n",
    "![](gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter in Gradient descent is the size of the steps, determined by the *learning rate* hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time:\n",
    "\n",
    "![](learning_rate_small.png)\n",
    "\n",
    "On the other hand, if the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up then you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution:\n",
    "\n",
    "![](learning_rate_large.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, not all cost function look like nice regular bowls. There may be holes, ridges, plateus, and all sorts of irregular terrains, making convergence to the minimum very difficult. The figure below shows the two main challenges with Gradient Descent: if the random initialization starts the algorithm on the left, then it will converge to a *local minimum*, which is not as good as the global minimum. If  it starts on the right, then it will take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum.\n",
    "\n",
    "![](gradient_descent_pitfalls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, the MSE cost function for a Linear Regression model happens to be a *convex function*. There are no local minima, just one global minimum. It is also a continous function with a slope that never changes abruptly. These two facts have a great consequence: Gradien Descent is guaranteed to approach arbitrarily close to the global minimum (if you wait long enough and if the learning rate is not too high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales. The below figure shows Gradient Descent on a training set where features $1$ and $2$ have the same scale (on the left), and on a training set where feature $1$ has much smaller values than feature $2$ (on the right).\n",
    "\n",
    "![](gradient_descent_with_without_feature_scaling.png)\n",
    "\n",
    "As you can see, on the left the Gradient Descent algorithm goes straight toward the\n",
    "minimum, thereby reaching it quickly, whereas on the right it first goes in a direction\n",
    "almost orthogonal to the direction of the global minimum, and it ends with a long\n",
    "march down an almost flat valley. It will eventually reach the minimum, but it will\n",
    "take a long time.\n",
    "\n",
    "**Tip**: When using Gradient Descent, you should ensure that all features\n",
    "have a similar scale (e.g., using Scikit-Learn’s StandardScaler\n",
    "class), or else it will take much longer to converge.\n",
    "\n",
    "This diagram also illustrates the fact that training a model means searching for a\n",
    "combination of model parameters that minimizes a cost function (over the training\n",
    "set). It is a search in the model’s parameter space: the more parameters a model has,\n",
    "the more dimensions this space has, and the harder the search is: searching for a needle\n",
    "in a 300-dimensional haystack is much trickier than in three dimensions. Fortunately,\n",
    "since the cost function is convex in the case of Linear Regression, the needle is\n",
    "simply at the bottom of the bowl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "To implement Gradient Descent, you need to compute the gradient of the cost function with regards to each model parameter $\\theta_j$. In other words, you need to calculate how much the cost function will change if you change $\\theta_j$ just a little bit. For this you need to compute the *partial derivatives*\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} \\text{MSE}(\\theta) = \\frac{2}{m} \\sum_{i=1}^{m} (\\theta^T x^{(i)} - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "Instead of computing these partial derivatives individually, you can compute them all in one go. The gradient vector, noted $c$ contains all the partial derivatives of the cost function (one for each model parameter).\n",
    "\n",
    "\n",
    "$$\\nabla_{\\theta} \\text{MSE}(\\theta) = [\\frac{\\partial}{\\partial \\theta_0} \\text{MSE}(\\theta), \\dots, \\frac{\\partial}{\\partial \\theta_n} \\text{MSE}(\\theta)]^T = \\frac{2}{m} X^T (X \\theta - y).$$\n",
    "\n",
    "Notice that this formula involves calculations over the full training set $X$, at each Gradient Descent step! This is why the algorithm is called *Batch Gradient Descent*: it uses the whole batch of training data every step. As a result it is terribly slow on very large training sets (but we will see much faster Gradient Descent algorithms shortly). However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation or SVD decomposition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means substracting $\\nabla_{\\theta} \\text{MSE}(\\theta)$ from $\\theta$. This is where the learning rate $\\eta$ comes into play: multiply the gradient vector by $\\eta$ bto determine the size of the downhill step\n",
    "\n",
    "$$\\theta^{\\text{(next step)}} = \\theta - \\eta \\nabla_{\\theta} \\text{MSE}(\\theta).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 #learning rate\n",
    "\n",
    "n_iterations = 1000\n",
    "\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.92675368],\n",
       "       [3.15085769]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's exactly what the Normal Equation found! Gradient Descent worked perfectly. Buy what if you used a different learning rate eta? The figure below shows the first $10$ steps of the Gradient Descent using three different learning rates (the dashed line represent the starting point).\n",
    "\n",
    "![](gradient_descent_various_learning_rates.png)\n",
    "\n",
    "On the left, the learning rate is too slow: the alogirthm will eventually reach the solution, but it will take a long time. In the middle, the learning rate looks pretty good: in just a few iterations, it has already converges to the solution. On the right, the learning rate is too high: the algorithm diverges, jumping all over the place and actually getting further and further away from the solution at every step.\n",
    "\n",
    "To find a good learning rate, you can use grid search. However, you may  However, you may want to limit the number of iterations so that grid search can eliminate models that take too long to converge.\n",
    "\n",
    "You may wonder how to set the number of iterations. If it is too low, you will still be far away from the optimal solution when the algorithm stops, but if it is too high, you will waste time while the model parameters do not change anymore. A simple solution\n",
    "is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes tiny—that is, when its norm becomes smaller than a tiny number $\\epsilon$ (called the tolerance)—because this happens when Gradient Descent has\n",
    "(almost) reached the minimum.\n",
    "\n",
    "**Convergence Rate**\n",
    "When the cost function is convex and its slope does not change abruptly (as is the case for the MSE cost function), Batch Gradient Descent with a fixed learning rate will eventually converge to the optimal solution, but you may have to wait a while: it can take $O(1/\\epsilon)$ iterations to reach the optimum within a range of ϵ depending on the\n",
    "shape of the cost function. If you divide the tolerance by $10$ to have a more precise solution, then the algorithm may have to run about $10$ times longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "The main problem with Batch Gradien Descent is the fact is uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, *Stochastic Gradient Descent* just picks a random instance in the training set at every step and computes the gradients based only on the single instance. Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration.  It also it makes possible to train on huge training sets, since only one instance needs to be in memory at each iteration. \n",
    "\n",
    "On the other hand, due to its stochastic (that is, random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently drecreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Overtimme it will end up very close to the minimum, but once it gets there it will continue to bounce aroun, never settling down (see figure below). So once the alogorithm stops, the final parameter values are good, but not optimal.\n",
    "\n",
    "![](stochastic_gradient_descent.png)\n",
    "\n",
    "When the cost function is very irregular, this can actually help the algorithm jump out of local minima, so Stochastic Gradient Descent has a better chance of finding the global minimum than Batch Gradient Descent does.\n",
    "\n",
    "Therefore randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. The function that determines the learning rate at each iteration is called the *learning schedule*. If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. If the the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early.\n",
    "\n",
    "This code implements Stochastic Gradient Descent using a simple learning schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)\n",
    "\n",
    "theta = np.random.randn(2,1) # random initilization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 *xi.T.dot(xi.dot(theta)-yi)\n",
    "        eta = learning_schedule(epoch*m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention we iterate by rounds of *m* iterations; each  round is called an *epoch*. While the Batch Gradient Descent code iterated $1,000$ times through the while training set, this code goes through the training set only $50$ times and reaches a fairly good solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.93179689],\n",
       "       [3.15054154]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the first $20$ steps of training (notice how irregular the steps are).\n",
    "\n",
    "![](sgd_20_steps.png)\n",
    "\n",
    "Notice that since instances are picked randomly, some instance may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set (make sure to shuffle the input features and the labels jointly), then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly.\n",
    "\n",
    "When using Stochastic Gradient Descent, the training instances\n",
    "must be independent and identically distributed (IID), to ensure\n",
    "that the parameters get pulled towards the global optimum, on\n",
    "average. A simple way to ensure this is to shuffle the instances during\n",
    "training (e.g., pick each instance randomly, or shuffle the training\n",
    "set at the beginning of each epoch). If you do not do this, for\n",
    "example if the instances are sorted by label, then SGD will start by\n",
    "optimizing for one label, then the next, and so on, and it will not\n",
    "settle close to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Linear Regression using SGD with Scikit-Learn you can use the SGDRegressor class, which defaults to optimizing the square error cost function. The following code runs for maximum $1000$ epoches (max_iter = $1000$) or until the loss drops by less than 1e-3 during one epoch (tol = 1e-3), starting with a learning rate of $0.1$ (eta0=0.1), using the default learning schedule (different from the preceding one), and it does not use any regularization (penalty = None; more details on this shortly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter = 1000, tol = 1e-3, penalty = None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you find a solution quite close to the one returned to the Normal Equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.87609182]), array([3.14488825]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "\n",
    "The last Gradient Descent alogorithm we will look at is called *Mini-batch Gradient Descent.* It is quite simple to understand once you know Batch and Stochastic Gradient Descent: at each step, instead of computing the gradients based on full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Minibatch GD computes the gradients on small random sets of instances called *mini-batches*. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = 0\n",
    "for epoch in range(n_iterations):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.85524776],\n",
       "       [3.0764481 ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm’s progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike Linear Regression as we saw earlier). The figure below shows the paths taken by the three Gradient Descent algorithms in parameter space during training. They all end up near the minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, don’t forget that Batch GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD would also reach the minimum if you used a good learning\n",
    "schedule.\n",
    "\n",
    "![](gradient_descent_paths_parameter_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Regression**\n",
    "\n",
    "What if your data is actually more complex than a simple straight line? Suprisingly, you can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called *Polynomial Regression*.\n",
    "\n",
    "Let's look at an example. First, let's generate some nonlinear data, based on a simple quadratic equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6*np.random.rand(m,1)-3\n",
    "y = 0.5* X**2 + X + 2 + np.random.randn(m,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, a straight line will never fit this data properly. So let's use Scikit-Learn's PolynomialFeatures class to transform our training data, adding the square (2^nd-degree polynomial) of each feature in the training set as new features (in this case thre is just one feature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.38942838])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree =2, include_bias = False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.38942838, 5.709368  ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_poly now contains the orginal feature of $X$ plus the square of this feature. Now you can fit a LinearRegression model to this extended training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.9735233]), array([[0.95038538, 0.52577032]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but to be expected as we introduced Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when there are multiple features, Polynomial Regression is capable of finding relationships between features (which is something a plain Linear Regression model cannot do). This is made possible by the fact that PolynomialFeatures also adds all combinations of features up to the given degree. For example, if there were two features $a$ and $b$, PolynomialFeatures with degree=3 would not only add the features $a^2, a^3, b^2$ and $b^3$ but also the combinations $ab, a^b$ and $ab^2$.\n",
    "\n",
    "PolynomialFeatures(degree=d) transforms an array containing $n$ features into an array containing $\\frac{(n+d)!}{d! n!}$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "If you perform high-degree Polynomial Regression, you will likely fit the training data much better than with plain LinearRegression. For example, the figure applies a $300$-degree polynomial model to the preceding training data, and compares the result with a pure linear model and a quadratic model (2^nd-degree polynomial). Notice how the $300$-degree polynomial model wiggles around to get as closed a possible to the training instances.\n",
    "\n",
    "![](high_degree_polynomial_regression.png)\n",
    "\n",
    "Of course, this high-degree Polynomial Regression model is severely overfitting the training data, while the linear model is underfitting it. The model that will generalize best in this case is the quadratic mmodel. It makes sense since the data was generated using a quadratic model, but in general you won't know the function that generated the data, so how can you decide how complex your model should be? How can you tell that your model is overfitting or underfitting the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use cross-validation to get an estimate of a model's generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when when a model is too simple or too complex.\n",
    "\n",
    "Another way is to look at the *learning curves*: these are plots of the model's performance on the training set and the validation set as a function of the traning set size (or the training iteration). To generate the plots, simply train the model several times on different sized subsets of the training set. The following code defines a function that plots the learning curves of a model given some training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X,y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2)\n",
    "    train_errors, val_errors = [],[]\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val,y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth = 3, label = \"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the learning curves of the plain Linear Regression model (a straight line;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Training set size\", fontsize=14) \n",
    "plt.ylabel(\"RMSE\", fontsize=14)  \n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.axis([0, 80, 0, 3])                     \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the performance on the training data: when there are just one or two instances in the training set, the model can fit them perfectly, which is why the curve starts at zero. But as new instances are addeed to the training set, it becomes impossible for the model to fit the training data perfectly, both because the data is noisy and because it is not linear at all. So the error on the training data goes up until it reaches a plateau, at which point adding new instances to the training set doesn't make the average error much better or worse. Now let's look at the performance of the model on the validation data. When the model is trained on very few training instances, it is incapable of generalizing properly, which is why the validation error is initially quite big. Then as the model is shown more training examples, it learns and thus the validation error slowly goes down. However, once again a straight line cannot do a good job modeling the data, so the error ends up a plateau, very close to the other curve.\n",
    "\n",
    "These learning curves are typical of an underfitting  model. Both curves have a reached a plateau; they are close and fairly high.\n",
    "\n",
    "Now let's look at the learning curves of a 10th-degree polynomial model on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.axis([0, 80, 0, 3])           \n",
    "plt.show()                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These learning curves look a bit like the previous ones, but there are two very important differences:\n",
    "\n",
    "- The error on the training data is much lower than with the Linear Regression model.\n",
    "\n",
    "- There is a gap between the curves. This means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer.\n",
    "\n",
    "**Tip** One way to improve an overfitting model is to feed it more training\n",
    "data until the validation error reaches the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias/Variance Tradeoff\n",
    "\n",
    "An important theoretical result of statistics and Machine Learning is the fact that a model's generalization error can be expressed as the sum of three very different errors:\n",
    "\n",
    "*Bias*\n",
    "\n",
    "This part of the generalization error is due to wrong assumption, and such as assuming that the data is linear when it is actually quadratic. A high-bias model ist most likely to underfit the training data.\n",
    "(This notion of bias is not to be confused with the bias term of linear models.)\n",
    "\n",
    "*Variance*\n",
    "\n",
    "This part is due to the model's excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial\n",
    "model) is likely to have high variance, and thus to overfit the training\n",
    "data.\n",
    "\n",
    "*Irreducible error*\n",
    "\n",
    "This part is due to the noisiness of the data itself. The only way to reduce this\n",
    "part of the error is to clean up the data (e.g., fix the data sources, such as broken\n",
    "sensors, or detect and remove outliers).\n",
    "\n",
    "Increasing a model’s complexity will typically increase its variance and reduce its bias.\n",
    "Conversely, reducing a model’s complexity increases its bias and reduces its variance.\n",
    "This is why it is called a tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Models\n",
    "\n",
    "A good way to reduce overfitting is to regularize the model (that is, to constraint it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a linear model, regularization is typically achieved by constraining the weights of the model. We will not look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "*Ridge regression* is a regularized version of Linear Regression: a *regularization term* equal to $\\alpha \\sum_{i=1}^{n} \\theta_i^2$ is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added tot he cost function during training. Once the model is trained, you want to evaluate the model's performance using the unregularized performance measure.\n",
    "\n",
    "The hyperparameter $\\alpha$ controls how much you want to regularize the model. If $\\alpha = 0$ then Ridge Regression is just Linear Regression. If $\\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data's mean. \n",
    "\n",
    "Ridge Regression cost function:\n",
    "\n",
    "$$J(\\theta) = \\text{MSE}(\\theta) + \\alpha \\frac{1}{2} \\sum_{i=1}^{n} \\theta_i^2.$$\n",
    "\n",
    "Note that the bias term $\\theta_0$ is not regularized. If we define $w$ as the vector of feature weights ($\\theta_1$ to $\\theta_n$), then the regularization term is simply equal to $\\frac{1}{2}( \\lvert \\lvert w \\rvert \\rvert_2)^2$. For Gradient Descent, just add $\\alpha w$ to the MSE gradient vector.\n",
    "\n",
    "It is important to scale the data (e.g., using a StandardScaler) before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.\n",
    "\n",
    "\n",
    "The below figure shows Ridge models trained on some linear data using different $\\alpha$ value. On the left, plain Ridge models are used, leading to linear predictions. On the right, the data is first expanded using PolynomialFeatures(degree = 10), then it scaled using a StandardScaler, and finally the Ridge models are applied to the resulting features: this is Polynomial Regression with Ridge regularization. Note how increasing $\\alpha$ leads to flatter (that is less extreme, more reasonable) predictions; this reduces the model's variance but increases its bias.\n",
    "\n",
    "![](ridge_regression.png)\n",
    "\n",
    "As with Linear Regression, we can perform Ridge Regression either by computing a closed-form equation or by performing Gradient Descent. The pros and cons are the same. The below equation shows the closed-form solution (where $A$ is the $(n+1) \\times (n+1)$) *identity matrix* except with a $0$ in the top-left cell, corresponding to the bias term).\n",
    "\n",
    "$$\\hat{\\theta} = (X^T X + \\alpha A)^{-1} X^T y$$\n",
    "\n",
    "Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, solver='cholesky')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge(alpha=1, solver = \"cholesky\")\n",
    "ridge_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.13357033]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using Stochastic Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.12759575])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(penalty = \"l2\")\n",
    "sgd_reg.fit(X,y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function equal to half the square of the $l_2$ norm of the weight vector: this is simply Ridge\n",
    "Regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "*Least Absolute Shrinkage and Selection Operator Regression* (simply called Lasso *Regression*) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularized term to the cost function, but it uses the $l_1$ norm of the weight vector instead of half the square of the $l_2$ norm\n",
    "\n",
    "$$J(\\theta) = \\text{MSE}(\\theta) + \\alpha \\sum_{i=1}^{n} \\lvert \\theta_i \\rvert$$\n",
    "\n",
    "The following figure shows the same thing as the above figure but replaces Ridge models with Lasso models and uses smaller $\\alpha$ values.\n",
    "\n",
    "![](lasso_regression.png)\n",
    "\n",
    "An important characteristic of Lasso Regression is that it tends to completely eliminate teh weights of the least important features (that is, set them to zero). For example, the dashed line in the right plot (with $\\alpha = 10^{-7}$) looks quadratic, almost linear: all the weights for the high-degree polynomial features are equal to zero. In other wrods, Lasso Regression automatically performs feature selection and outputs a *sparse model* (that is, with few nonzero feature weights)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a sense of why this is the case by looking at the below figure:\n",
    "\n",
    "![](lasso_regression_versus_ridge_regularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on the top-left plot, the background countous (ellipses) represent an unregularized MSE cost function ($\\alpha=0$), and the white circle show the Batch Gradient Descent path with that cost function. The foreground countours (diamonds) represent the $l_1$ penalty, and the triangles show the BGD path for this penalty only ($\\alpha \\rightarrow \\infty$). Notice how the path first reaches $\\theta_1 = 0$ then rolls down a gutter until it reaches $\\theta_2 = 0$. On the top right plot, the contours represent the same cost function plus an $l_1$ penalty with $\\alpha = 0.5$. The global minimum is on the $\\theta_2 = 0$ axis. BGD first reaches $\\theta_2 = 0$, then rolls down the glutter until it reaches the global minimum. The two bottom plots show the same thing but uses an $l_2$ penalty instead. The regularized minimum is closer to $\\theta = 0$ than the unregularized minimum, but the weights do not get fully eliminated.\n",
    "\n",
    "On the Lasso cost function, the BGD path tends to bounce across the gutter toward the end. This is because the slope changes abruptly at $\\theta_2 = 0$. You need to gradually reduce the learning rate in order to actually converge to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso cost function is not differentiable at $\\theta_i = 0$ (for $i = 1,2,..,n)$, but Gradient Descent still works fine if you use a *subgradient vector* $g$ instead when any $\\theta_i = 0$. The below equation shows a subgradient vector question you can use for Gradient Descent with the Lasso cost function.\n",
    "\n",
    "Lasso Regression subgradient vector\n",
    "\n",
    "$$g(\\theta, J) = \\theta_{\\theta} MSE(\\theta) + \\alpha [\\text{sign}(\\theta_1) \\cdots \\text{sign}(\\theta_1) ]^T$$\n",
    "\n",
    "where $$\\text{sign}(\\theta_i) = \\begin{cases} -1 \\text{ if } \\theta_1 < 0 \\\\ 0 \\ \\ \\ \\text{  if } \\theta_i = 0 \\\\ +1 \\text{ if } \\theta_i > 0 \\end{cases}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small Scikit-Learn example using the Lasso class. Note that you could instead use an SGDRegressor (penalty = \"$l1$\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.09481859])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha = 0.1)\n",
    "lasso_reg.fit(X,y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELastic Net\n",
    "\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso's regularization terms, and you can control the mix ratio $r$. When $r=0$, Elastic Net is equivalent to Ridge Regression and when $r=1$, it is equivalent to Lasso Regression.\n",
    "\n",
    "$$J(\\theta) = \\text{MSE}(\\theta) + r \\alpha \\sum_{i=1}^{n} \\lvert \\theta_i \\rvert + \\frac{1-r}{2} \\alpha \\sum_{i=1}^{n} \\theta_i^2.$$\n",
    "\n",
    "So when should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good\n",
    "default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\n",
    "may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "Here is a short example using Scikit-Learn’s ElasticNet ($l1$_ratio corresponds to\n",
    "the mix ratio $r$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.09373477])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio = 0.5)\n",
    "elastic_net.fit(X,y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called *early stopping*. The below figure shows a complex model (in this case a high-degree\n",
    "Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set naturally goes down, and so does its prediction error on the validation set. However, after a while the validation error stops decreasing and actually starts to go back up.\n",
    "This indicates that the model has started to overfit the training data. With early stopping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a “beautiful free lunch.”\n",
    "\n",
    "![](early_stopping_regularization.png)\n",
    "\n",
    "With Stochastic and Mini-batch Gradient Descent, the curves are\n",
    "not so smooth, and it may be hard to know whether you have\n",
    "reached the minimum or not. One solution is to stop only after the\n",
    "validation error has been above the minimum for some time (when\n",
    "you are confident that the model will not do any better), then roll\n",
    "back the model parameters to the point where the validation error\n",
    "was at a minimum.\n",
    "\n",
    "Here is a basic implementation of early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = deepcopy(sgd_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with warm_start=True, when the fit() method is called, it just continues training where it left off instead or restarting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "As we discussed, some regression algorithms  can be used for classification as well (and vice versa). *Logistic Regression* (also called *Logit Regression*) is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than $50%$, then the model predicts that the instance belongs to the class (called the positive class, labeled \"1\"), or else it predicts that it does not (that is, it belongs to the negative class, labeled \"0\"). This makes it a binary classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Probabilities\n",
    "\n",
    "So how does it work? Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input feature (plus a bias term), but instead of outputting the result directly like Linear Regression model does, it outputs the *logistic* of this result\n",
    "\n",
    "\n",
    "$$\\hat{p} = h_{\\theta} = \\sigma (x^T \\theta).$$\n",
    "\n",
    "The logistic-noted $\\sigma(\\cdot)$ - is a *sigmoid function* (that is, $S$-shaped) that outputs a number between $0$ and $1$. It is defined as\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + \\exp(-t)}.$$\n",
    "\n",
    "![](logistic_regression.png)\n",
    "\n",
    "Once the Logistic Regression model has estimated the $\\hat{p} = h_{\\theta} (x)$ that an instance $x$ belongs to the positive class, it can make its prediction $\\hat{y}$ easily\n",
    "\n",
    "$$\\hat{y} = \\begin{cases}  0 \\text{ if } \\hat{p} < 0.5 \\\\ 1 \\text{ if } \\hat{p} \\geq 0.5. \\end{cases}$$\n",
    "\n",
    "Notice that $\\sigma(t) < 0.5$ when $t < 0$, and $\\sigma(t) \\geq 0.5$ when $t \\geq 0 $, so a Logistic Regression model predicts $1$ if $x^T \\theta$ is positive, and $0$ if it is negative.\n",
    "\n",
    "The score $t$ is often called the *logit*: this name comes from the fact that the logit function, defined as $\\text{logit}(p) = \\log(p/(1-p))$, is the inverse of the logistic function. Indeed, if you compute the logit of the estimated probability $p$, you will find the result is $t$. The logit is also called *log-odds*, since it is the log of the ratio between the estimated probability for the positive class and the estimated probability for the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Training and Cost Function**\n",
    " \n",
    "Now you know a Logistic Regression model estimates probabilities and makes predictions. But how is it trained? The objective of training is to set the parameter vector $\\theta$ so that model estimates high probabilities for positive instances ($y=1$ ) and low probabilites for negative instances $y=0$. This idea is captured by the cost function shown belown for a single training instance $x$\n",
    "\n",
    "\n",
    "$$c(\\theta) = \\begin{cases}  -\\log( \\hat{p}) \\text{ if } y = 1 \\\\ -\\log(1- \\hat{p}) \\text{ if } y=0. \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cost function makes sense becauase $-\\log(t)$ grows very large when $t$ approaches $0$, so the cost will be large if the model estimates a probability close to $0$ for a positive instance, and it will also be large very large if the model estimates a probability close to $1$ for a negative instance. On the other hand, $-\\log(t)$ is close to $0$ when $t$ is closed to $1$, so the cost will be close to $1$ for a positive instance, which is precisely what we want.\n",
    "\n",
    "The cost function over the whole training set is simply the average cost over all training instances. It can be written in a single expression, called the *log loss*, show in equation below\n",
    "\n",
    "$$J(\\theta) = \\frac{-1}{m} \\sum_{i=1}{m} [y^{(i)} \\log(\\hat{p}^{(i)})+ (1-y^{(i)})\\log(1 - \\hat{p}^{(i)})].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bad news is that there is no closed-form equation to compute the value of $\\theta$ that minimizes this cost function (there is no equivalent of the Normal Equation). But the good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning\n",
    "rate is not too large and you wait long enough). The partial derivatives of the cost function with regards to the $j$th model parameter $\\theta_j$ is given by\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\sigma (\\theta^T x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "Once you have the gradient vector containing all the partial derivatives you can use it in the Batch Gradient algorithm. That’s\n",
    "it: you now know how to train a Logistic Regression model. For Stochastic GD you would of course just take one instance at a time, and for Mini-batch GD you would use a mini-batch at a time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "\n",
    "Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica.\n",
    "\n",
    "Let's try to build a classifier to detect the Iris-Virginica type based only on the petal width feature. First let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=iris['data'][:,3:] #petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (iris['target'] == 2).astype(np.int) # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look ath model's estimated probabilities for flowers with petal widths varying from $0$ to $3$ cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new = np.linspace(0,3,1000).reshape(-1,1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:,1], \"g-\", label= \"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:,0],\"b--\", label= \"Not Iris-Virginica\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better figure:\n",
    "\n",
    "![](estimated_probabilities_decision_boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares) generally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of overlap.\n",
    "Above about 2 cm the classifier is highly confident that the flower is an Iris- Virginica (it outputs a high probability to that class), while below 1 cm it is highly confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica” class). In between these extremes, the classifier is unsure. However, if you ask it to\n",
    "predict the class (using the predict() method rather than the predict_proba() method), it will return whichever class is the most likely. Therefore, there is a decision\n",
    "boundary at around 1.6 cm where both probabilities are equal to 50%: if the petal width is higher than 1.6 cm, the classifier will predict that the flower is an Iris-Virginica, or else it will predict that it is not (even if it is not very confident):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict([[1.7],[1.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure\n",
    "\n",
    "![image.png](linear_decision_boundary.png)\n",
    "\n",
    "shows the same dataset but this time displaying two features: petal width and length. Once trained, the Logistic Regression classifier can estimate the probability that a new flower is an Iris-Virginica based on these two features. The dashed line represents the points where the model estimates a $50$% probability: this is the model's decision boundary. Note that it is a linear boundary. Each parallel line represents the points where the model outputs a specific probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the other linear models, Logistic Regression models can be regularized using $l_1$ or $l_2$ penalties. Scikit-Learn actually adds an $l_2$ penalty by default.\n",
    "\n",
    "The hyperparameter controlling the regularization strength of a Scikit-Learn LogisticRegression model is not alpha (as in other linear models), but its inverse: $C$. The higher the value of $C$, the *less* the model is regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression\n",
    "\n",
    "The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine mulitple binary classifiers. This is called *Softmax Regression* or *Multinomial Logistic Regression.*\n",
    "\n",
    "The idea is quite simple: when given an instance $x$, the Softmax Regression model first computes a score $s_k(x)$ for each class $k$, then estimates the probability of each class by applying the *softmax function* (also called the *normalized exponential*) to the scores. The equation to compute $s_k(x)$ should look familiar, as it is just like  the equation for Linear Regression prediction\n",
    "\n",
    "$$s_k(x) = x^T \\theta^{(k)} \\text{ (Softmax score for class $k$)}$$\n",
    "\n",
    "Note that each class has its own dedicated parameter vector $\\theta^{(k)}$. All these vectors are typically stored as rows in a *parameter matrix* $\\Theta$.\n",
    "\n",
    "Once you computed the score of every class for the instance $x$, you can estimate the probability $\\hat{p}_k$ that the instance belongs to class $k$ by running the scores through the softmax function: \n",
    "\n",
    "$$\\hat{p}_k = \\sigma(s(x))_k  = \\frac{\\exp(s_k(x))}{\\sum_{j=1}^{K} \\exp(s_j(x))}$$\n",
    "\n",
    "- $K$ is the number of classes.\n",
    "\n",
    "- $s(x)$ is a vector containing the scores of each class for the instance $x$.\n",
    "\n",
    "- $\\sigma(s(x))_k$ is the estimated probability that the instance $x$ belongs to class $k$ given the scores of each class for that instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the Logistic Regression classifier, the Softmax Regresison classifier predicts the class with the highest probability (which is imply the class with the highest score), as show in the below equation\n",
    "\n",
    "$$\\hat{y} = \\text{argmax}_k \\sigma(s(x))_k = \\text{argmax}_k s_k(x) = \\text{argmax}_k ((\\theta^{(k)})^Tx) $$\n",
    "\n",
    "- The *argmax* operator retuns the value of a variable that maximizes a function.\n",
    "\n",
    "**Note:** The Softmax Regression classifier predicts only one class at a time (i.e., it is multiclass, not multioutput) so it should be used only with mutually exclusive classes such as different types of plants. You\n",
    "cannot use it to recognize multiple people in one picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how the model estimates probabilities and makes predictions, let's look at training. The objective is to have a model that estimates a high probability for the target class (and consequently a low probability for the other classes). Minimizing the cost function show below, called the *cross entropy*, should lead to this objective because penalizes the model when it estimates a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimted class probabilites match the target classes (we will use it again several times in the following chapters).\n",
    "\n",
    "$$J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log (\\hat{p}_{k}^{(i)})$$\n",
    "\n",
    "- $y_k^{(i)}$ is the target probability that the $i$th instance belongs to class $k$. In general, it is either equal to $1$ or $0$, depending on whether the instance belongs to the class or not.\n",
    "\n",
    "Notice that when there are just two classes ($K=2$), this cost function is equivalent to the Logistic Regression's cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient vector of this cost function with regards to $\\theta^{(k)}$ is given by the equation below\n",
    "\n",
    "$$\\nabla_{\\theta^{(k)}} J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{p}_k^{(i)} - y_k^{(i)})x^{(i)}$$\n",
    "\n",
    "Now you can compute the gradient vector for every class, then use Gradient Descent (or any other optimization algorithm) to find the paramter matrix $\\Theta$ that minimizes the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Softmax Regression to classify iris flowers into all three classes. Scikit-Learn's LogisticRegression uses one-versus-all by default when you train it on more than two classes, but you can set the multi_class hyperparameter to \"multinomial\" to switch it to Softmax Regression instead. You must also specify a solver that supports Softmax Regression, such as the \"lbfgs\" solver. It also applies $l_2$ regularization by default, which you can control using the hyperparameter $C$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data'][:,(2,3)] # petal length, petal width\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, multi_class='multinomial')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg = LogisticRegression(multi_class = \"multinomial\", solver =\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict([[5,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict_proba([[5,2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the resulting decision boundaries, represented by the background colors. Notice that the decision boundaries between any two classes are linear. The figure also shows the probabilities for the Iris-Versicolor class, represented by the curved lines (e.g., the line labeled with 0.450 represents the 45% probability boundary). Notice that the model can predict a class that has estimated probability below $50$%. For example, at the point where all decision boundaries meet, all classses have an equal estimated probability of 33%.\n",
    "\n",
    "![](softmax_regression_decision_boundaries.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
