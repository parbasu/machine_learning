{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear SVM Classification**\n",
    "\n",
    "The fundamental idea behind SVMs is best with some pictures. The figure below shows part of the iris dataset that was introduced earlier. The two classes can clearly be separated easily with a straight line (they are *linearly separable*). The left plot shows the decision boundaries of three possible linear classifiers. The model whose decision boundary is represented by the dashed line is so bad that it does not even separate the classes properly. The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these models will probably not perform well on new instances. In contrast, the solid line plot on the right represents the decision boundary of an SVM classifer; this line not only separates the two classes but also stays as far away from the closes training instances as possible. You can think of an SVM classifer as fitting the widest possible street (represented by the parellel dashed lines) between the classes. This is called *large margin classification*.\n",
    "\n",
    "![](large_margin_classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that adding more training instances \"off the street\" will not affect the decision boundary at all: it is fully determined (or \"supported\") by the instances located on the edge of the street. These instances are called *support vectors* (they are circle above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are sensitive to the feature scales, as you can see in the below figure: on the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. After feature scaling (e.g., using Scikit-Learn's StandardScaler), the decision boundary looks much better (on the right plot).\n",
    "\n",
    "![](sensitivity_feature_scales.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin Classification\n",
    "\n",
    "If we strictly impose that all instances be off the street and on the right side, this is called *hard margin classification*. There are two main issues with hard margin classification. First, it only works if the data is linearly separable, and second it is quite sensitive to outliers. Figure below shows the iris dataset with just one outlier: on the left, it is impossible to find a hard margin, and on the right the decision boundary ends up very different from the one we saw above without the outlier, and it will probably not generalize as well.\n",
    "\n",
    "![](hard_margin_sensitivity_outliers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid these issues it is preferable to use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the *margin violations* (that is, instances that end up in the middle of the street or even on the wrong side). This is called *soft margin classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit-Learn's SVM classes, you can control this balance using the $C$ hyperparmeter: a smaller $C$ leads to a wider street but more margin violations. Figure below shows the decision boundaries and margins of two soft margin SVM classifiers on a nonlinearly separable dataset. On the left, using a low $C$ value the margin is quite large, but many instances end up on the street. On the right, using a high $C$ value the classifier makes fewer margin violations but ends up with a smaller margin. However, it seems that the first classifier will generalize better: in fact even on this training set it makes fewer predicition errors, since most of the margin violations are actually on the correct side of the decision boundary.\n",
    "\n",
    "![](large_margin_versus_fewer_margin_violations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip.** If your SVM model is overfitting, you can try regularizing it by reducing $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear SVM model (using the LinearSVC class with $C=1$ and the *hinge loss* function, described shortly) to detect Iris-Viriginica flowers. The resulting model is represents on the left of the above figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:,(2,3)] # petal length, petal width\n",
    "y = (iris['target'] == 2).astype(np.float64) # Iris-Virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_svc', LinearSVC(C=1, loss='hinge'))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5,1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression classifiers, SVM classifiers do not output\n",
    "probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could use the SVC class, using SVC(kernel = 'linear', C=1), but it is much slower, especially with large training sets, so it is not recommended. Another option is to use the SGDClassifier class, with SGDClassifier(loss='hinge', alpha=1/(m * C)). This applies regular Stochastic Gradient Descent to train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it can be useful to handle huge datasets that do not fit in memory (out-of-core training), or to handle online classification tasks.\n",
    "\n",
    "The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. This is automatic if you scale the data using the StandardScaler. Moreover, make sure you set the loss hyperparameter to \"hinge\", as it is not the default value. Finally, for better performance you should set the dual hyperparameter to False, unless there are more features than training instances (we will discuss later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "\n",
    "Although linear SVM classifiers are efficient and work suprisingly well in many cases, many datasets are not even close to being linearly seperable. One approach to handling nonlinear datasets is to add more features, such as polynomial features; in some cases this can result in a linearly separable dataset. Consider the left plot in the below figure: it represents a simple dataset with just one feature $x_1$. This dataset is not linearly separable. But if you add a second feature $x_2 = (x_1)^2$, the resulting $2$D dataset is perfectly linearly separable.\n",
    "\n",
    "![](adding_features_to_make_dataset_linearly_separable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this idea using Scikit-Learn you can create a Pipeline containing a PolynomialFeatures transformer, followed by a StandardScaler and a LinearSVC. Let's test this on the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles (see below figure). You can generate this dataset using the make_moons() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree = 3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C=10, loss='hinge'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](linearSVM_classifier_polynomial_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel\n",
    "\n",
    "Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning alogorithms (not just SVMs), but at a low polynomial degree it cannot deal with very complex datasets, and with a high polynomial degree it creates and huge number of features, making the model too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, when using SVMs you can apply a mathematical technique called *kernel trick*. It makes it possible to get the same result as if you added many polynomial features, even with very high-degree polynomials, without actually having to add them. So there is no combinatorial explosion of the number of features since you don't actually add any features. This trick is implemented by the SVC class. Let's test it on the moons dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel = 'poly', degree=3, coef0 = 1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains an SVM classifier using a $3$rd-degree polynomial kernel. It is represented on the left of below figure. On the right is another SVM classifier using a $10$th-degree polynomial kernel. Obviously, if your model is overfitting, you might want to reduced the polynomial degree. Conversely, if it is underfitting, you can try increasing it. The hyperparameter coef0 controls how much the model is influence by high-degree polynomials versus low-dgree polynomials.\n",
    "\n",
    "![](svm_classifiers_polynomial_kernels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to find the right hyperparameter values is to\n",
    "use grid search . It is often faster to first do a very\n",
    "coarse grid search, then a finer grid search around the best values\n",
    "found. Having a good sense of what each hyperparameter actually\n",
    "does can also help you search in the right part of the hyperparameter\n",
    "space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Similarity Features\n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed using a *similarity function* that measures how each instance resembles a particular *landmark*. For example, let's take the one-dimensional dataset discussed earlier and add two landmarks to it at $x_1 = -2$ and $x_1 = 1$ (see the left plot in below figure). Next, let's define the similarity function to be the Gaussian *Radial Basis Function (RBF)* with $\\gamma = 0.3$\n",
    "\n",
    "$$\\phi_{\\gamma} (x, l) = \\exp(-\\gamma \\lvert \\lvert - x - l \\rvert \\rvert^2)$$\n",
    "\n",
    "It is a bell_shaped function varying from $0$ (very far away from the larmark) to $1$ (at the landmark). Now we are ready to compute the new features. For example, let's look at the instance $x_1 = -1$: it is located at a distance of $1$ from the first landmark, and $2$ from the second landmark. Therefore its new features are $x_2 = \\exp(-0.3^2 \\times 1^2) \\approx 0.74$ and $x_3 = \\exp(-0.3 \\times 2^2) \\approx 0.30$. The plot on the right of below figure shows the transformed dataset (dropping the original features). As you can see, it is now lienarly seperable.\n",
    "\n",
    "![](similarity_features_gaussian_rbf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder how to select the landmarks. The simplest approach is to create a landmark at the location of each and every instance in the dataset. This creates many dimensions and thus increases the chances that the transformed training set will be linearly separable. The downside is that a training set with m instances and n features\n",
    "gets transformed into a training set with m instances and m features (assuming you drop the original features). If your training set is very large, you end up with an equally large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian RBF Kernel\n",
    "\n",
    "Just like the polynomial features method, the similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive to compute all the additional features, especially on large training sets. However, once again the kernel trick does its SVM magic: it makes it possible to obtain a similar result as if you had added many similarity features, without actually having to add them. Let’s try the Gaussian RBF kernel using the SVC class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('svm_clf', SVC(kernel='rbf', gamma=5, C=0.001))])\n",
    "rbf_kernel_svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is reprented on the bottom left of the below figure. The other plots show models trained with different values of hyperparameters gamma ($\\gamma$) and $C$. Increasing gamma makes the bell-shape curve narrower (see the left plot of below figure), and as a result each instance's range of influence is smaller: the decision boundary ends up being more irregular, wiggling around invidual instances. Conversely, a small gamma value makes the bell-shaped curve wider, so instances have a large range of influence, and the decision boundary ends up smoother. So $\\gamma$ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it, and if it is underfitting, you should increase it (similar to the $C$ hyperparameter).\n",
    "\n",
    "![](svm_classifiers_rbf_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other kernels exist but are used much more rarely. For example, some kernels are specialized for specific data structures. *String kernels* are sometimes used when classifying text documents or DNA sequences.\n",
    "\n",
    "With so many kernels to choose from, how can you decide which\n",
    "one to use? As a rule of thumb, you should always try the linear\n",
    "kernel first (remember that LinearSVC is much faster than SVC(ker\n",
    "nel=\"linear\")), especially if the training set is very large or if it\n",
    "has plenty of features. If the training set is not too large, you should try the Gaussian RBF kernel as well; it works well in most cases. Then if you have spare time and computing power, you can also\n",
    "experiment with a few other kernels using cross-validation and grid\n",
    "search, especially if there are kernels specialized for your training\n",
    "set’s data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Regression**\n",
    "\n",
    "As we mentioned earlier, the SVM algorithm is quite versatile: not only doest it support linear and nonlinear classification, but also linear and nonlinear regression. The trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (that is, instances off the street). The width of the street is controlled by a hyperparameter $\\epsilon$.  The figure shows two linear SVM Regression models trained on some random linear data, one with a large margin ($\\epsilon = 1.5$) and the other small margin $(\\epsilon = 0.5)$\n",
    "\n",
    "![](svm_regression.png)\n",
    "\n",
    "Adding more instances within the margin does not affect the model's predictions; thus the model is said to be $\\epsilon$-insenstive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Scikit-Learn's LinearSVR class to perform linear SVM Regression. The following code produces the model represented on the left of the above figure (the training data should be scaled and centered first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon = 1.5)\n",
    "svm_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, you use a kernalized SVM model. For example, the figure below shows SVM Regression on a random quadratic training set, using a $2$nd-degree polynomial kernel. There is little regularization on the left plot (that is, a large $C$ value), and much more regularization on the right plot (that is, a small $C$ value).\n",
    "\n",
    "![](svm_regression_2nd_degree_polynomial_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code produces the model represented on the left of above figure using Scikit-Learn's SVR class (which supports the kernel trick). The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is the regression equivalent of the LinearSVR class. The LinearSVR class scales linearly with the size of the training set (just like the LinearSVR class), while the SVR class gets much too slow when the training set grows large (just like the SVC class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel = 'poly', degree=2, C=100, epsilon = 0.1)\n",
    "svm_poly_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs can also be used for outlier detection; see Scikit-Learn’s documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood\n",
    "\n",
    "Earlier we used the convention of putting all the model parameters in one vector $\\theta$, including the bias term $\\theta_0$ and the input feature weights $\\theta_1$ to $\\theta_n$, and adding a bias input $x_0 = 1$ to all instances. In this section, we will use a different convention, which is more convenient when you are dealing with SVMs: the bias term will be called $b$ and the feature weights vector will be called $w$. No bias feature will be added to the input feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Function and Predictions\n",
    "\n",
    "The linear SVM classifier model predicts the class of a new instance $x$ by simply computing the decision function $w^T x + b = w_1 x_1 + \\cdots w_n x_n  + b$: if the result is positive, the predicted class $\\hat{y}$ is the postive class $(1)$, or else it is the negative clas $(0)$; see below equation\n",
    "\n",
    "$$ \\hat{y} = \\begin{cases} 0 \\text{ if } w^T x + b < 0 \\\\ 1 \\text{ if } w^T x + b \\geq 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below figure shows the decision function that corresponds to the model on the left of the figure earlier: it is a two-dimensional plane since this dataset has two features (petal width and petal length). The decision boundary is the set of points where the decision function is equal to $0$: it is the intersection of two planes, which is a straight line (represented by the thick solid line). The dashed lines represent the points where the decision function is equal to $1$ or $-1$: they are parallel and at equal distance to the decision boundary, forming a margin around it. Training a linear SVM classifier means finding the value of $w$ and $b$ that makes this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (softmargin)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Objective**\n",
    "\n",
    "Consider the slope of the decision function: it is equal to the norm of the weight vector $\\| w \\|$. If we define this slope by $2$, the points where the decision function is equal to $+1,-1$ are going to be twice as far away from the decision boundary. In other words, dividing the slope by $2$ will increase multiply the margin by $2$. This is easier to visualize in $2$D in the below figure. The smaller the weight vector $w$, the larger the margin.\n",
    "\n",
    "![](smaller_weight_vector_larger_margin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to minimize $\\| w \\|$ to get a large margin. However, if we also want to avoid any margin violation (hard margin), then we need the decision function to be greater than $1$ for all positive training instances, and lower than $-1$ for negative training instances. If we define $t^{(i)} = -1$ for negative instances (if $y^{(i)} = 0$) and $t^{(i)} = 1$ for positive instances (if $y^{(i)}=1$), then we can express this constraint as $t^{(i)} (w^T x^{(i)} + b) \\geq 1$ for all instances.\n",
    "\n",
    "We can therefore express the hard margin linear SVM classifier objective as the *contrained optimization problem* in equation below\n",
    "\n",
    "$$\\text{minimize}_{w,b} \\ \\frac{1}{2} w^T w$$\n",
    "\n",
    "$$\\text{subject to } t^{(i)}(w^T x^{(i)} + b) \\geq 1 \\text{ for } i=1,2,\\dots, m$$\n",
    "\n",
    "We are minimizing $\\frac{1}{2}w^T w$ which is equal to $\\frac{1}{2} \\| w \\|^2$, rather than minimizing $\\| w\\|$. Indeed, $\\frac{1}{2} \\| w \\|^2$ has a nice and simple derivative (it is just $w$) while $\\| w \\|$ is not differentiable at $w=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the soft margin objective, we need to introduce a *slack variable* $\\zeta^{(i)} \\geq 0$ for each instance: $\\zeta^{(i)}$ measures how much the $i$th instance is allowed to violate the margin. We now have two conflicting objectives: making the slack variable as small as possible to reduced the margin violations, and making $\\frac{1}{2} \\| w \\|^2$ as small as possible to increase the margin. This is where the $C$ hyperparameter comes in: it allows us to define the trade-off between these two objectives. This gives us the following contrained optimization problem \n",
    "\n",
    "$$\\text{minimize}_{w,b, \\zeta} \\ \\frac{1}{2} w^T w + C \\sum_{i=1}^{m} \\zeta^{(i)}$$\n",
    "\n",
    "$$\\text{subject to } t^{(i)}(w^T x^{(i)} + b) \\geq 1 - \\zeta^{(i)} \\text{ and } \\zeta^{(i)} \\geq 0  \\text{ for } i=1,2,\\dots, m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Programming\n",
    "\n",
    "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as *Quadratic Programming* (QP) problems. The general problem formulation is given by the following equation\n",
    "\n",
    "$$ \\text{minimize}_{p} \\ \\frac{1}{2} p^T H p + f^T p $$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$Ap \\leq b$$\n",
    "\n",
    "where $p$ is an $n_p$-dimensional vector ($n_p$ = number of parameters), $H$ is an $n_p \\times n_p $ matrix, $f$ is an $n_p$-dimensional vector, $A$ is an $n_c \\times n_p$ matrix ($n_c = $ number of constraints), $b$ is an $n_c$-dimensional vector.\n",
    "\n",
    "Note that expression $Ap \\leq b$ acutally defines $n_c$ constraints: $p^T a^{(i)} \\leq b^{(i)}$ for $i=1,2, \\dots, n_c$, where $a^{(i)}$ is the vector containing the elements of the $i$th row of $A$ and $b^{(i)}$ is the $i$th element of $b$.\n",
    "\n",
    "You can easily verify that if you set the QP parameters in the following way, you get the hard margin linear SVM classifier objective:\n",
    "\n",
    "- $n_p = n+1$, where $n$ is the number of features (+$1$ is for the bias term)\n",
    "\n",
    "- $n_c = m$, where $m$ is the number of training instances.\n",
    "\n",
    "- $H$ is the $n_p \\times n_p$ identity matrix, except with a zero in the top-left cell (to ignore the bias term).\n",
    "\n",
    "- $f=0$, an $n_p$-dimensional vector full of $0$s.\n",
    "\n",
    "- $b=-1$, an $n_c$-dimensional vector full of $-1$s.\n",
    "\n",
    "- $a^{(i)} = - t^{(i)} \\dot{x}^{(i)}$, where $\\dot{x}^{(i)}$ with an extra bias feature $\\dot{x}_0 = 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one way to train a hard margin linear SVM classifier is just to use an off-the-shelf QP solver by passing it the preceeding parameters. The resulting vector $p$ will contain the bias term $b=p_0$ and the feature weights $w_i = p_i$ for $i=1,2, \\dots, n$. Similarly, you can use a QP solver to solve the soft margin problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dual Problem\n",
    "\n",
    "Given a constrained optimization problem, known as the *primal problem*, it is possible to express a different but closely related problem, called its *dual problem*. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions it can even have the same solutions as the primal problem. Luckily, the SVM problem happends to meet these conditions, so can choose to solve the primal problem or the dual problem; both will have the same solution. The below equation shows the dual form of the linear SVM objective\n",
    "\n",
    "$$\\text{minimize}_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} {x^{(i)}}^T x^{(j)}- \\sum_{i=1}^{m} \\alpha^{(i)}$$\n",
    "\n",
    "subject to $\\alpha^{(i)} \\geq 0$ for $i=1, 2, \\dots, m$.\n",
    "\n",
    "Once you find the vector $\\hat{\\alpha}$ that minimizes this equation (using a QP solver), you can compute $\\hat{w}$ and $\\hat{b}$ that minimize the primal problem by using the following equaiton\n",
    "\n",
    "$$\\hat{w} = \\sum_{i=1}^{m} \\hat{\\alpha}^{(i)} t^{(i)} x^{(i)}$$\n",
    "\n",
    "$$\\hat{b} = \\frac{1}{n_s} \\sum_{i=1}^{m} (t^{(i)} - \\hat{w}^T x^{(i)}) \\ (\\hat{\\alpha}^{(i)} > 0)$$ \n",
    "\n",
    "The dual problem is faster to solver than the primal when the number of training instances is smaller than the number of features. More importantly, it makes the kernel trick possible, while the primal does not. So what is this kernel trick anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelized SVM\n",
    "\n",
    "Suppose you want to apply a $2$nd-degree polynomial transformation to a two-dimensional training set (such as the moons training set), then train a linear SVM classifier on the transformed training set. The below shows the $2$nd-degree polynomial mapping function $\\phi$ that you want to apply.\n",
    "\n",
    "\n",
    "$$\\phi(x) = \\phi((x_1, x_2)^T)) = (x_1^2 , \\sqrt{2} x_1 x_2, x_2^2)$$\n",
    "\n",
    "Notice that the transformed vector is three-dimensional instead of two-dimensional. Now one can show that $\\phi(a)^T \\phi(b) = (a^T b)^2$. The dot product of the transformed vectors is equal to the square of the dot product of the original vectors.\n",
    "\n",
    "Now here is the key insight: if you apply transformation $\\phi$ to all training instances, the dual problem will contain the dot product $ \\phi(x^{(i)})^T \\phi(x^{(j)})$. But if $\\phi$ is the $2$nd-degree polynomial transformation defined above, then you can replace the dot product of transformed vectors simply by $({x^{(i)}}^T x^{(j)})^2$. So you don't actually need to transform the training instances at all: just replace the dot porduct by its square in the above dual problem equation. The result will be strictly the same as if you went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick make the whole process much more computationally efficient. This is the essence of the kernel trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $K(a,b) = (a^T b)^2$ is called a $2$nd-degree *polynomial kernel*. In Machine Learning is a function capable of computing the dot product $\\phi(a)^T \\phi(b)$ based only on the original vectors $a$ and $b$, without having to compute (or even to know about) the transformation $\\phi$. Here are some commonly used kernels:\n",
    "\n",
    "- Linear: $K(a,b) = a^T b $\n",
    "\n",
    "- Polynomial: $K(a,b) = (\\gamma a^T b +r)^d$\n",
    "\n",
    "- Gaussian RBF: $K(a,b) = \\exp(-\\gamma \\| a-b\\|^2)$\n",
    "\n",
    "- Sigmoid: $K(a,b) = \\tanh (\\gamma a^T b + r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mercer's Theorem**\n",
    "\n",
    "According to Mercer’s theorem, if a function $K(a, b)$ respects a few mathematical conditions called Mercer’s conditions ($K$ must be continuous, symmetric in its arguments so $K(a, b) = K(b, a)$, etc.), then there exists a function $ϕ$ that maps a and b into another space (possibly with much higher dimensions) such that $K(a, b) = ϕ(a)T ϕ(b)$. So you can use $K$ as a kernel since you know $ϕ$ exists, even if you don’t know what $ϕ$ is. In the case of the Gaussian RBF kernel, it can be shown that $ϕ$ actually maps each training instance to an infinite-dimensional space, so it’s a good thing you don’t need\n",
    "to actually perform the mapping! Note that some frequently used kernels (such as the Sigmoid kernel) don’t respect all\n",
    "of Mercer’s conditions, yet they generally work well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still loose end we must tie. To go from the dual solution to the primal solution in the case of a linear SVM classifier, if you apply the kernel trick you end up with equations that include $\\phi(x^{(i)})$. In fact $\\hat{w}$ must have the same number of dimensions as $\\phi(x^{(i)})$, which may be huge or even infinite, so you can't compute. But how can you make predictions without knowing $\\hat{w}$? Well, the good news is that you can plug in the formula for $\\hat{w}$ from the earlier equation into the decision function for a new instance $x^{(n)}$ and you get an equation with only dot products between input vectors. This makes it possible to use the kernel trick, once again\n",
    "\n",
    "$$h_{\\hat{w}, \\hat{b}} (\\phi(x^{(n)})) = \\hat{w}^T \\phi(x^{(n)}) + \\hat{b} = (\\sum_{i=1}^{m} \\hat{\\alpha}^{(i)} t^{(i)} \\phi(x^{(i)}))^T \\phi(x^{(n)}) + \\hat{b}$$\n",
    "\n",
    "which is ultimately equal to\n",
    "\n",
    "$$ \\sum_{i=1}^{m} \\hat{\\alpha}^{(i)} t^{(i)} K(x^{i}, x^{(n)}) + \\hat{b}  \\ \\ (\\hat{\\alpha}^{(i)} > 0)$$\n",
    "\n",
    "Note that since $\\alpha^{(i)} \\neq 0$ only for support vectors, making predictions involves computing the dot products of the new input vector $x^{(n)}$ with only the support vectors, not all the training instances. Of course, you also need to compute the bias term $\\hat{b}$, using the same trick\n",
    "\n",
    "$$\\hat{b} = \\frac{1}{n_s} \\sum_{i=1}^{m} (t^{(i)}- \\sum_{j=1}^{m} \\hat{\\alpha}^{(j)} t^{(j)} K(x^{(i)}, x^{(j)}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
