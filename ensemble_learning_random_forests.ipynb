{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have trained a few classifiers, each one achieving about $80$% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see figure below).\n",
    "\n",
    "![](training_diverse_classifiers.png)\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a *hard voting* classifier (see figure below)\n",
    "\n",
    "![](hard_voting_classifier_predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat suprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a *weak learner* (meaning it does only slighlty better than random guessing), the ensemble can still be a *strong learner* (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this possible? The following analogy can help shed some light on this mystery. Suppose you have a slightly biased coin that has a $51$% chance of coming up heads, and $49$% chance of coming up tails. If you toss it $1000$ times, you will generally get more less $510$ heads and 490 tails, and hence a majority heads. If you do the math, you will find that the probability of obtaining a majority of heads after $1000$ tosses is close to $75$%. The more you toss the coin, the higher the probability (e.g., with $10000$ tosses, the probability climbs over $97$%). This is due to the *law of large numbers:* as you keep tossing the coin, the ratio of heads gets closer and closer to the probability of heads ($51$%). The below figure shows $10$ series of biased coin tosses. You can see that as the number of tosses increases, the ratio of heads approaches $51$%. Eventually all $10$ series end up so close to $51$% that they are consistenly above $50$%.\n",
    "\n",
    "![](law_of_large_numbers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, suppose you build an ensemble containing $1000$ classifiers that are indivdually correct only  $51$% of the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to $75$% accuracy! However, this only true if all the classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case since they are trained on the same data. They are likely to make the same type of errors, so there will be many majority votes for the wrong class, reducing the ensemble's acccuracy.\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent\n",
    "from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble’s accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates and trains a voting classifier in Scikit-Learn, composed of three diverse classifiers (the training set is the moons dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at each classifier's accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.896\n",
      "VotingClassifier 0.888\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting classifier slighlty outperforms all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If all classifiers are able to estimate class probabilites (that is, they have a predic_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called *soft voting*. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to is replace voting = \"hard\" with voting = \"soft\" and ensure that all classifiers can estimate class probabilies. This is not the case of the SVC class by default, so you need to set its probability hyperparameter to True (this will make the SVC class to use cross-validation to estimate class probabilities, slowing down training, and it will add a predict_proba() method). If you modify the preceding code to use soft voting, you will find that the voting classifier achieves over $91.2$% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "\n",
    "One way to get a diverse set of classifiers is to use a very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor, but to train them on different subsets of the training set. When sampling is performed with replacement, this method is called *bagging* (short for bootstrap aggregating). When sampling is performed without replacement, it is called *pasting*.\n",
    "\n",
    "In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor. This sampling and training process is represented in the figure below\n",
    "\n",
    "![](pasting_bagging_training_set_sampling_training.png)\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the *statistical mode* (that is the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "As you can see in the above figure predictors can all be trained in parallel, via different CPU cores or even different servers. Similarly, predictions can be made in parallel.\n",
    "This is one of the reasons why bagging and pasting are such popular methods: they scale very well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting in Scikit-Learn\n",
    "\n",
    "Scikit-Learn offers a simple API for both bagging and pasting with BaggingClassifier class (or BaggingRegressor for regression). The following code trains an ensemble of $500$ Decision Tree classifiers, each trained on $100$ training instances randomly sampled from the training set with replacement (this is an example of bagging, but if you want to use pasting method, just set bootstrap = False). The n_jobs parameters tells Scikit-Learn the number of CPU cores to use for training and predictions (-1 tells Scikit-Learn to use all available cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500, \n",
    "max_samples = 100, bootstrap = True, n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting\n",
    "instead of hard voting if the base classifier can estimate class probabilities\n",
    "(i.e., if it has a predict_proba() method), which is the case\n",
    "with Decision Trees classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below figure compares the decision boundary of single Decision Tree with the decision boundary of a bagging ensemble of $500$ trees (from the preceeding code), both trained on the moons dataset. As you can see, the ensemble's predictions will likley generalize much better than the sinlge Decision Tree's predictions: the ensemble has a comparable bais but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular).\n",
    "\n",
    "![](single_decision_tree_versus_bagging_ensemble_500_trees.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slighlty higher bias than pasting, but this also means that predictors end up being less correlated so the ensemble's variance is reduced. Overall, bagging often results in better models, which explains why it is generally preferred. However, if you have spare time and CPU power you can use cross-validation to evaluate both bagging and pasting and select the one that works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be samples several times for any given predictor, while others may not be sampled at all. By default a BaggingClassifier samples $m$ training instances with replacement (bootstrap = True), where $m$ is the size of the training set. This means only about $63$% of the training instances are sampled on average for each predictor. The remaining $37$% of the training instances that are not sampled are called *out-of-bag* (oob) instances. Note that they are not the same $37$% for all predictors.\n",
    "\n",
    "Since a predictor never seens the oob instances during training, it can be evaluated on these instances. without the need for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evulations of each predictor.\n",
    "\n",
    "In Scikit-Learn you can set oob_score = True when creating a BaggingClassifier to request an automatic oob evaluation after training. The following code demonstrates this. The resulting evaluation score is available through the oob_score_ variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators = 500,\n",
    "bootstrap = True, n_jobs = -1, oob_score = True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9013333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this oob evaluation, this BaggingClassifier is likley to achieve about $90.1$% accuracy on the test set. Let's verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The oob decision function for each training instance is also available through the oob_decision_function_ variable. In this case (since the base estimator has a predict_proba() method) the decision function returns the class proababilities for each training instance. For example, the oob evaluation estimates that the first training instance has a $58.82$% probability of belonging to the positive class (and $41.17$% of belonging to the negative class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35384615, 0.64615385],\n",
       "       [0.35233161, 0.64766839],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10650888, 0.89349112],\n",
       "       [0.39086294, 0.60913706],\n",
       "       [0.02824859, 0.97175141],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96855346, 0.03144654],\n",
       "       [0.75409836, 0.24590164],\n",
       "       [0.005     , 0.995     ],\n",
       "       [0.78918919, 0.21081081],\n",
       "       [0.86082474, 0.13917526],\n",
       "       [0.95628415, 0.04371585],\n",
       "       [0.05405405, 0.94594595],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98984772, 0.01015228],\n",
       "       [0.96666667, 0.03333333],\n",
       "       [0.99473684, 0.00526316],\n",
       "       [0.00990099, 0.99009901],\n",
       "       [0.34054054, 0.65945946],\n",
       "       [0.89893617, 0.10106383],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97073171, 0.02926829],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98984772, 0.01015228],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.69186047, 0.30813953],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14117647, 0.85882353],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.38785047, 0.61214953],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.2       , 0.8       ],\n",
       "       [0.3255814 , 0.6744186 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00649351, 0.99350649],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0060241 , 0.9939759 ],\n",
       "       [0.984375  , 0.015625  ],\n",
       "       [0.91397849, 0.08602151],\n",
       "       [0.93296089, 0.06703911],\n",
       "       [0.98404255, 0.01595745],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02923977, 0.97076023],\n",
       "       [0.98930481, 0.01069519],\n",
       "       [0.01183432, 0.98816568],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9895288 , 0.0104712 ],\n",
       "       [0.79651163, 0.20348837],\n",
       "       [0.4171123 , 0.5828877 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.65934066, 0.34065934],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88383838, 0.11616162],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66321244, 0.33678756],\n",
       "       [0.11518325, 0.88481675],\n",
       "       [0.62735849, 0.37264151],\n",
       "       [0.88888889, 0.11111111],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14851485, 0.85148515],\n",
       "       [0.88359788, 0.11640212],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05208333, 0.94791667],\n",
       "       [0.03867403, 0.96132597],\n",
       "       [0.37572254, 0.62427746],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85638298, 0.14361702],\n",
       "       [0.00649351, 0.99350649],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22488038, 0.77511962],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94252874, 0.05747126],\n",
       "       [0.77222222, 0.22777778],\n",
       "       [0.01169591, 0.98830409],\n",
       "       [1.        , 0.        ],\n",
       "       [0.17741935, 0.82258065],\n",
       "       [0.64739884, 0.35260116],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06214689, 0.93785311],\n",
       "       [0.48717949, 0.51282051],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01875   , 0.98125   ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27272727, 0.72727273],\n",
       "       [0.48850575, 0.51149425],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02162162, 0.97837838],\n",
       "       [0.97765363, 0.02234637],\n",
       "       [0.27135678, 0.72864322],\n",
       "       [0.92346939, 0.07653061],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.78804348, 0.21195652],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97927461, 0.02072539],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.95348837, 0.04651163],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.30412371, 0.69587629],\n",
       "       [0.93658537, 0.06341463],\n",
       "       [0.24064171, 0.75935829],\n",
       "       [0.99494949, 0.00505051],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.62091503, 0.37908497],\n",
       "       [0.35609756, 0.64390244],\n",
       "       [0.37254902, 0.62745098],\n",
       "       [0.86206897, 0.13793103],\n",
       "       [0.93641618, 0.06358382],\n",
       "       [0.07142857, 0.92857143],\n",
       "       [0.77108434, 0.22891566],\n",
       "       [0.01092896, 0.98907104],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03468208, 0.96531792],\n",
       "       [0.97311828, 0.02688172],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00515464, 0.99484536],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01604278, 0.98395722],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.953125  , 0.046875  ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99009901, 0.00990099],\n",
       "       [0.        , 1.        ],\n",
       "       [0.34545455, 0.65454545],\n",
       "       [0.25136612, 0.74863388],\n",
       "       [0.00574713, 0.99425287],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33862434, 0.66137566],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99530516, 0.00469484],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70792079, 0.29207921],\n",
       "       [0.9076087 , 0.0923913 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98901099, 0.01098901],\n",
       "       [0.99431818, 0.00568182],\n",
       "       [0.99447514, 0.00552486],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.1025641 , 0.8974359 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03174603, 0.96825397],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05084746, 0.94915254],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.92972973, 0.07027027],\n",
       "       [0.71165644, 0.28834356],\n",
       "       [0.54857143, 0.45142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16470588, 0.83529412],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96808511, 0.03191489],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00571429, 0.99428571],\n",
       "       [0.        , 1.        ],\n",
       "       [0.37222222, 0.62777778],\n",
       "       [0.90449438, 0.09550562],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01069519, 0.98930481],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97029703, 0.02970297],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.96666667, 0.03333333],\n",
       "       [0.82233503, 0.17766497],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09714286, 0.90285714],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02150538, 0.97849462],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07303371, 0.92696629],\n",
       "       [0.98765432, 0.01234568],\n",
       "       [0.80571429, 0.19428571],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89880952, 0.10119048],\n",
       "       [0.98461538, 0.01538462],\n",
       "       [0.14583333, 0.85416667],\n",
       "       [0.14917127, 0.85082873],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22167488, 0.77832512],\n",
       "       [0.93333333, 0.06666667],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98314607, 0.01685393],\n",
       "       [0.        , 1.        ],\n",
       "       [0.52061856, 0.47938144],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.09497207, 0.90502793],\n",
       "       [0.10638298, 0.89361702],\n",
       "       [0.98918919, 0.01081081],\n",
       "       [0.01714286, 0.98285714],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42211055, 0.57788945],\n",
       "       [0.0625    , 0.9375    ],\n",
       "       [0.57458564, 0.42541436],\n",
       "       [0.60106383, 0.39893617],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.61627907, 0.38372093],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22485207, 0.77514793],\n",
       "       [0.81770833, 0.18229167],\n",
       "       [0.08      , 0.92      ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82417582, 0.17582418],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09356725, 0.90643275],\n",
       "       [0.02840909, 0.97159091],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91282051, 0.08717949],\n",
       "       [0.14364641, 0.85635359],\n",
       "       [0.96132597, 0.03867403],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.68421053, 0.31578947],\n",
       "       [0.07368421, 0.92631579],\n",
       "       [0.98850575, 0.01149425],\n",
       "       [0.89361702, 0.10638298],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92696629, 0.07303371],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.31111111, 0.68888889],\n",
       "       [0.98907104, 0.01092896],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00588235, 0.99411765],\n",
       "       [0.81909548, 0.18090452],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.79393939, 0.20606061],\n",
       "       [0.94949495, 0.05050505],\n",
       "       [1.        , 0.        ],\n",
       "       [0.71356784, 0.28643216],\n",
       "       [0.53513514, 0.46486486],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91463415, 0.08536585],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89673913, 0.10326087],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.70053476, 0.29946524],\n",
       "       [0.07486631, 0.92513369],\n",
       "       [0.41463415, 0.58536585],\n",
       "       [0.18181818, 0.81818182],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84042553, 0.15957447],\n",
       "       [0.82165605, 0.17834395],\n",
       "       [0.01176471, 0.98823529],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02906977, 0.97093023],\n",
       "       [0.90449438, 0.09550562],\n",
       "       [0.94791667, 0.05208333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.55813953, 0.44186047],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.02673797, 0.97326203],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96132597, 0.03867403],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03389831, 0.96610169],\n",
       "       [0.00487805, 0.99512195],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01136364, 0.98863636],\n",
       "       [1.        , 0.        ],\n",
       "       [0.14973262, 0.85026738],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0047619 , 0.9952381 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.44019139, 0.55980861],\n",
       "       [0.06451613, 0.93548387],\n",
       "       [0.22485207, 0.77514793],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98029557, 0.01970443],\n",
       "       [0.18888889, 0.81111111],\n",
       "       [0.99425287, 0.00574713],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97883598, 0.02116402],\n",
       "       [0.25263158, 0.74736842],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98843931, 0.01156069],\n",
       "       [0.00520833, 0.99479167],\n",
       "       [0.03389831, 0.96610169],\n",
       "       [0.98979592, 0.01020408],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02185792, 0.97814208],\n",
       "       [0.68717949, 0.31282051]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "\n",
    "The BaggingClassifier class supports sampling the features as well, This is controlled by two hyperparameters: max_features and bootstrap_features. They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the *Random Patches method*. Keeping all the training instances (that is bootstrap = False and max_samples = 1.0) but sampling features (that is bootstrap_features = True and/or max_features smaller than 1.0) is called the *Random Subspaces method*.\n",
    "\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "As we have discussed, a Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for DecisionTrees (similary, there is a RandomForestRegressor class for regression tasks). The following code trains a Random Forest Classifier with $500$ trees (each limited to maximum $16$ nodes), using all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees\n",
    "\n",
    "When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make\n",
    "trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).\n",
    "\n",
    "A forest of such extremely random trees is simply called an Extremely Randomized Trees ensemble (or Extra-Trees for short). Once again, this trades more bias for a\n",
    "lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one\n",
    "of the most time-consuming tasks of growing a tree.\n",
    "\n",
    "You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\n",
    "class. Its API is identical to the RandomForestClassifier class. Similarly, the\n",
    "\n",
    "It is hard to tell in advance whether a RandomForestClassifier\n",
    "will perform better or worse than an ExtraTreesClassifier. Generally,\n",
    "the only way to know is to try both and compare them using\n",
    "cross-validation (and tuning the hyperparameters using grid\n",
    "search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Yet another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by\n",
    "looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each\n",
    "node’s weight is equal to the number of training samples that are associated with it.\n",
    "\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. You can access the\n",
    "result using the feature_importances_ variable. For example, the following code trains a RandomForestClassifier on the iris dataset and outputs each feature’s importance. It seems that the most important features are the\n",
    "petal length (44%) and width (42%), while sepal length and width are rather unimportant in comparison (11% and 2%, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09897975576214006\n",
      "sepal width (cm) 0.024563451621514045\n",
      "petal length (cm) 0.42810844826272715\n",
      "petal width (cm) 0.4483483443536187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary, if you train a Random Forest classifier on the MNIST dataset and plot each pixel's importance, you get the image represented in the below figure\n",
    "\n",
    "![](mnist_pixel_importance_random_forest_classifier.png)\n",
    "\n",
    "Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each try to correct its predecessor. There are many boosting methods available, but by far the most popular are *AdaBoost* (short for Adaptive Boosting) and Gradient Boosting. Let's start with AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
    "\n",
    "For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set, A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on (see below figure).\n",
    "\n",
    "![](adaboost_sequential_training_instance_weight_updates.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the decision boundaries of five consecutive predictors on the moons dataset (in this example, each predictor is a highly regularzied SVM classifier with an RBF kernel). The first classifier gets many instances wrong, so their weights get boosted. The second classifier therefore does a better job on these instances, and so on. The plot on the right represents the same sequence of predictors except that the learning rate is halved (that is, the misclassified instance weights are boosted half as much at every iteration). As you can see, this sequential learning technique has some similarities with Gradient Descent, except that instead of tweaking a single predictor's parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better.\n",
    "\n",
    "![](decision_boundaries_consecutive_predictors.png)\n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.\n",
    "\n",
    "There is one important drawback to this sequential learning technique: it cannot be parallelized (or only partially), since each predictor can only be trained after the previous predictor has been\n",
    "trained and evaluated. As a result, it does not scale as well as bagging or pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the AdaBoost algorithm. Each instance weight $w^{(i)}$ is initially set to $\\frac{1}{m}$. A first predictor is trained and its weight error rate $r_1$ is computed on the training set; see below equation\n",
    "\n",
    "$$r_j = \\frac{\\sum_{i=1}^{m} w^{(i)} \\ (\\hat{y}_{j}^{(i)} \\neq y^{(i)})}{\\sum_{i=1}^{m} w^{(i)}}$$\n",
    "\n",
    "where $\\hat{y}_{j}^{(i)}$ is the $j$th predictor's prediction for the $i$th instance.\n",
    "\n",
    "The predictor's weight $\\alpha_j$ is then computed using the below equation\n",
    "\n",
    "$$\\alpha_j = \\eta \\log (\\frac{1-r_j}{r_j})$$\n",
    "\n",
    "where $\\eta$ is learning rate hyperparameter (defaults to $1$). The more accurate the predictor is, the highers its  weight will be. If it just guessing randomly, then its weight will be close to zero. However, if it is most often wrong (that is, less accurate than random guessing) then its weight will be negative.\n",
    "\n",
    "Next the instance weights are updating using the below equation: the misclassified instances are boosted.\n",
    "\n",
    "for $i=1,2, \\dots, m$\n",
    "\n",
    "$$w^{(i)} \\leftarrow \\begin{cases} w^{(i)} \\text{ if } \\hat{y}_{j}^{(i)} = y^{(i)}  \\\\ w^{(i)} \\ \\exp(\\alpha_j) \\text{ o/w } \\end{cases} $$\n",
    "\n",
    "Then all instance weights are normalized (that is, divided by $\\sum_{i=1}^{m} w^{(i)}$).\n",
    "\n",
    "Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor's weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "\n",
    "To make predictions, AdaBoost simply computes the predictions of all the predictors and weights them using the predictor weights $\\alpha_j$. The predicted class is the one that recieves the majority of the weighted votes (see below equation)\n",
    "\n",
    "$$\\hat{y}(x) = \\text{argmax}_{k} \\sum_{j=1}^{N} \\alpha_j \\ (\\hat{y}_j (x) = k)$$\n",
    "\n",
    "where $N$ is the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which stands for Stagewise Additive Modeling using a Multiclass Exponential loss function). When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the predictors can estimate class probabilities (i.e., if they have a predict_proba() method), Scikit-Learn can use a variant of SAMME called SAMME.R (the R stands for “Real”), which relies on class probabilities rather than predictions and generally\n",
    "performs better.\n",
    "\n",
    "The following code trains an AdaBoost classifier based on 200 Decision Stumps using\n",
    "Scikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an Ada\n",
    "BoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1—in\n",
    "other words, a tree composed of a single decision node plus two leaf nodes. This is\n",
    "the default base estimator for the AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your AdaBoost ensemble is overfitting the training set, you can\n",
    "try reducing the number of estimators or more strongly regularizing\n",
    "the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Another very popular Boosting algorithm is Gradient Boosting. Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
    "correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
    "\n",
    "Let’s go through a simple regression example using Decision Trees as the base predictors\n",
    "(of course Gradient Boosting also works great with regression tasks). This is\n",
    "called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let’s\n",
    "fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic training\n",
    "set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a second DecisionTreeRegressor on the residual errors made by the frist predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y -tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a third regressor on the residual errors made by the second predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg3.fit(X,y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure\n",
    "\n",
    "![](gradient_boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "represents the predictions of these three trees in the left column, and the ensemble’s predictions in the right column. In the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. In the second\n",
    "row, a new tree is trained on the residual errors of the first tree. On the right you can see that the ensemble’s predictions are equal to the sum of the predictions of the first\n",
    "two trees. Similarly, in the third row another tree is trained on the residual errors of the second tree. You can see that the ensemble’s predictions gradually get better as\n",
    "trees are added to the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe\n",
    "gressor class. Much like the RandomForestRegressor class, it has hyperparameters to\n",
    "control the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on),\n",
    "as well as hyperparameters to control the ensemble training, such as the number of\n",
    "trees (n_estimators). The following code creates the same ensemble as the previous\n",
    "one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, such as $0.1$, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called *shrinkage*. The figure below shows two GBRT ensembles trained with a low learning rate: the one on the left does not have enough trees to fit the training set, while the one on the right has too many trees and overfits the training set.\n",
    "\n",
    "![](gbrt_ensembles_not_enough_predictors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal number of trees, you can use early stopping. A simple way to implement this is to use the staged_predict() method: it returns an iterator over the predictions made by the ensemble at each stage of training (with one tree, two trees, etc.). The following code trains a GBRT ensemble with $120$ trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=84)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation errors are represented on the left of below figure, and the best model's predictions are represented on the right.\n",
    "\n",
    "![](tuning_the_number_of_trees_early_stopping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to implement early stopping by actually stopping training early (instead of training a large number of trees first and then looking back to find the optimal number). You can do so by setting warm_start=True, which makes Scikit- Learn keep existing trees when the fit() method is called, allowing incremental training. The following code stops training when the validation error does not improve for five iterations in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation MSE: 0.002750279033345716\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum validation MSE:\", min_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientBoostingRegressor class also supports a subsample hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training each tree. For\n",
    "example, if subsample=0.25, then each tree is trained on 25% of the training instances,\n",
    "selected randomly. As you can probably guess by now, this trades a higher bias\n",
    "for a lower variance. It also speeds up training considerably. This technique is called\n",
    "Stochastic Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use Gradient Boosting with other cost functions.\n",
    "This is controlled by the loss hyperparameter (see Scikit-Learn’s\n",
    "documentation for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For XGBoost use Google Collab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking \n",
    "\n",
    "Skipping for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
